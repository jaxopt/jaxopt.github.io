{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Deep Equilibrium (DEQ) model in Flax with Anderson acceleration.\n\nThis implementation is strongly inspired by the Pytorch code snippets in [3].\n\nA similar model called \"implicit deep learning\" is also proposed in [2].\n\nIn practice BatchNormalization and initialization of weights in convolutions are\nimportant to ensure convergence.\n\n[1] Bai, S., Kolter, J.Z. and Koltun, V., 2019. Deep Equilibrium Models.\nAdvances in Neural Information Processing Systems, 32, pp.690-701.\n\n[2] El Ghaoui, L., Gu, F., Travacca, B., Askari, A. and Tsai, A., 2021.\nImplicit deep learning. SIAM Journal on Mathematics of Data Science, 3(3),\npp.930-958.\n\n[3] http://implicit-layers-tutorial.org/deep_equilibrium_models/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from functools import partial\nfrom typing import Any, Mapping, Tuple, Callable\n\nfrom absl import app\nfrom absl import flags\n\nimport flax\nfrom flax import linen as nn\n\nimport jax\nimport jax.numpy as jnp\nfrom jax.tree_util import tree_structure\n\nimport jaxopt\nfrom jaxopt import loss\nfrom jaxopt import OptaxSolver\nfrom jaxopt import FixedPointIteration\nfrom jaxopt import AndersonAcceleration\nfrom jaxopt.linear_solve import solve_gmres, solve_normal_cg\nfrom jaxopt.tree_util import tree_add, tree_sub, tree_l2_norm\n\nimport optax\n\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\nfrom collections import namedtuple\n\n\ndataset_names = [\n    \"mnist\", \"kmnist\", \"emnist\", \"fashion_mnist\", \"cifar10\", \"cifar100\"\n]\n\n# training hyper-parameters\nflags.DEFINE_float(\"l2reg\", 0., \"L2 regularization.\")\nflags.DEFINE_float(\"learning_rate\", 0.001, \"Learning rate.\")\nflags.DEFINE_integer(\"maxiter\", 100, \"Maximum number of iterations.\")\nflags.DEFINE_enum(\"dataset\", \"mnist\", dataset_names, \"Dataset to train on.\")\nflags.DEFINE_integer(\"net_width\", 1, \"Multiplicator of neural network width.\")\nflags.DEFINE_integer(\"evaluation_frequency\", 1,\n                     \"Number of iterations between two evaluation measures.\")\nflags.DEFINE_integer(\"train_batch_size\", 256, \"Batch size at train time.\")\nflags.DEFINE_integer(\"test_batch_size\", 1024, \"Batch size at test time.\")\n\n\nsolvers = [\"normal_cg\", \"gmres\", \"anderson\"]\nflags.DEFINE_enum(\"backward_solver\", \"normal_cg\", solvers,\n                  \"Solver of linear sytem in implicit differentiation.\")\n\n# anderson acceleration parameters\nflags.DEFINE_enum(\"forward_solver\", \"anderson\", [\"anderson\", \"fixed_point\"],\n                  \"Whether to use Anderson acceleration.\")\nflags.DEFINE_integer(\"forward_maxiter\", 20, \"Number of fixed point iterations.\")\nflags.DEFINE_float(\"forward_tol\", 1e-2, \"Tolerance in fixed point iterations.\")\nflags.DEFINE_integer(\"anderson_history_size\", 5,\n                     \"Size of history in Anderson updates.\")\nflags.DEFINE_float(\"anderson_ridge\", 1e-4,\n                   \"Ridge regularization in Anderson updates.\")\n\nFLAGS = flags.FLAGS\n\n\ndef load_dataset(split, *, is_training, batch_size):\n  \"\"\"Loads the dataset as a generator of batches.\"\"\"\n  ds, ds_info = tfds.load(f\"{FLAGS.dataset}:3.*.*\", split=split,\n                          as_supervised=True, with_info=True)\n  ds = ds.cache().repeat()\n  if is_training:\n    ds = ds.shuffle(10 * batch_size, seed=0)\n  ds = ds.batch(batch_size)\n  return iter(tfds.as_numpy(ds)), ds_info\n\n\nclass ResNetBlock(nn.Module):\n  \"\"\"ResNet block.\"\"\"\n\n  channels: int\n  channels_bottleneck: int\n  num_groups: int = 8\n  kernel_size: Tuple[int, int] = (3, 3)\n  use_bias: bool = False\n  act: Callable = nn.relu\n\n  @nn.compact\n  def __call__(self, z, x):\n    # Note that stddev=0.01 is important to avoid divergence.\n    # Empirically it ensures that fixed point iterations converge.\n    y = z\n    y = nn.Conv(features=self.channels_bottleneck, kernel_size=self.kernel_size,\n                padding=\"SAME\", use_bias=self.use_bias,\n                kernel_init=jax.nn.initializers.normal(stddev=0.01))(y)\n    y = self.act(y)\n    y = nn.GroupNorm(num_groups=self.num_groups)(y)\n    y = nn.Conv(features=self.channels, kernel_size=self.kernel_size,\n                padding=\"SAME\", use_bias=self.use_bias,\n                kernel_init=jax.nn.initializers.normal(stddev=0.01))(y)\n    y = y + x\n    y = nn.GroupNorm(num_groups=self.num_groups)(y)\n    y = y + z\n    y = self.act(y)\n    y = nn.GroupNorm(num_groups=self.num_groups)(y)\n    return y\n\n\nclass DEQFixedPoint(nn.Module):\n  \"\"\"Batched computation of ``block`` using ``fixed_point_solver``.\"\"\"\n\n  block: Any  # nn.Module\n  fixed_point_solver: Any  # AndersonAcceleration or FixedPointIteration\n\n  @nn.compact\n  def __call__(self, x):\n    # shape of a single example\n    init = lambda rng, x: self.block.init(rng, x[0], x[0])\n    block_params = self.param(\"block_params\", init, x)\n\n    def block_apply(z, x, block_params):\n      return self.block.apply(block_params, z, x)\n\n    solver = self.fixed_point_solver(fixed_point_fun=block_apply)\n    def batch_run(x, block_params):\n      return solver.run(x, x, block_params)[0]\n\n    # We use vmap since we want to compute the fixed point separately for each\n    # example in the batch.\n    return jax.vmap(batch_run, in_axes=(0,None), out_axes=0)(x, block_params)\n\n\nclass FullDEQ(nn.Module):\n  \"\"\"DEQ model.\"\"\"\n\n  num_classes: int\n  channels: int\n  channels_bottleneck: int\n  fixed_point_solver: Callable\n\n  @nn.compact\n  def __call__(self, x, train):\n    # Note that x is a batch of examples:\n    # because of BatchNorm we cannot define the forward pass in the network for\n    # a single example.\n    x = nn.Conv(features=self.channels, kernel_size=(3,3), use_bias=True,\n                padding=\"SAME\")(x)\n    x = nn.BatchNorm(use_running_average=not train, momentum=0.9,\n                     epsilon=1e-5)(x)\n    block = ResNetBlock(self.channels, self.channels_bottleneck)\n    deq_fixed_point = DEQFixedPoint(block, self.fixed_point_solver)\n    x = deq_fixed_point(x)\n    x = nn.BatchNorm(use_running_average=not train, momentum=0.9,\n                     epsilon=1e-5)(x)\n    x = nn.avg_pool(x, window_shape=(8,8), padding=\"SAME\")\n    x = x.reshape(x.shape[:-3] + (-1,))  # flatten\n    x = nn.Dense(self.num_classes)(x)\n    return x\n\n\n# For completeness, we also allow Anderson acceleration for solving\n# the implicit differentiation linear system occurring in the backward pass.\ndef solve_linear_system_fixed_point(matvec, v):\n  \"\"\"Solve linear system matvec(u) = v.\n\n  The solution u* of the system is the fixed point of:\n    T(u) = matvec(u) + u - v\n  \"\"\"\n  def fixed_point_fun(u):\n    d_1_T_transpose_u = tree_add(matvec(u), u)\n    return tree_sub(d_1_T_transpose_u, v)\n\n  aa = AndersonAcceleration(fixed_point_fun,\n                            history_size=FLAGS.anderson_history_size, tol=1e-2,\n                            ridge=FLAGS.anderson_ridge, maxiter=20)\n  return aa.run(v)[0]\n\n\ndef main(argv):\n  del argv\n\n  # Hide any GPUs from TensorFlow. Otherwise TF might reserve memory and make\n  # it unavailable to JAX.\n  tf.config.experimental.set_visible_devices([], 'GPU')\n\n  # Solver used for implicit differentiation (backward pass).\n  if FLAGS.backward_solver == \"normal_cg\":\n    implicit_solver = partial(solve_normal_cg, tol=1e-2, maxiter=20)\n  elif FLAGS.backward_solver == \"gmres\":\n    implicit_solver = partial(solve_gmres, tol=1e-2, maxiter=20)\n  elif FLAGS.backward_solver == \"anderson\":\n    implicit_solver = solve_linear_system_fixed_point\n\n  # Solver used for fixed point resolution (forward pass).\n  if FLAGS.forward_solver == \"anderson\":\n    fixed_point_solver = partial(AndersonAcceleration,\n                                 history_size=FLAGS.anderson_history_size,\n                                 ridge=FLAGS.anderson_ridge,\n                                 maxiter=FLAGS.forward_maxiter,\n                                 tol=FLAGS.forward_tol, implicit_diff=True,\n                                 implicit_diff_solve=implicit_solver)\n  else:\n    fixed_point_solver = partial(FixedPointIteration,\n                                 maxiter=FLAGS.forward_maxiter,\n                                 tol=FLAGS.forward_tol, implicit_diff=True,\n                                 implicit_diff_solve=implicit_solver)\n\n  train_ds, ds_info = load_dataset(\"train\", is_training=True,\n                                    batch_size=FLAGS.train_batch_size)\n  test_ds, _ = load_dataset(\"test\", is_training=False,\n                            batch_size=FLAGS.test_batch_size)\n  input_shape = (1,) + ds_info.features[\"image\"].shape\n  num_classes = ds_info.features[\"label\"].num_classes\n\n  net = FullDEQ(num_classes, 3 * 8 * FLAGS.net_width, 4 * 8 * FLAGS.net_width,\n                fixed_point_solver)\n\n  def predict(all_params, images, train=False):\n    \"\"\"Forward pass in the network on the images.\"\"\"\n    x = images.astype(jnp.float32) / 255.\n    mutable = [\"batch_stats\"] if train else False\n    return net.apply(all_params, x, train=train, mutable=mutable)\n\n  logistic_loss = jax.vmap(loss.multiclass_logistic_loss)\n\n  def loss_from_logits(params, l2reg, logits, labels):\n    sqnorm = tree_l2_norm(params, squared=True)\n    mean_loss = jnp.mean(logistic_loss(labels, logits))\n    return mean_loss + 0.5 * l2reg * sqnorm\n\n  @jax.jit\n  def accuracy_and_loss(params, l2reg, data, aux):\n    all_vars = {\"params\": params, \"batch_stats\": aux}\n    images, labels = data\n    logits = predict(all_vars, images)\n    accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == labels)\n    loss = loss_from_logits(params, l2reg, logits, labels)\n    return accuracy, loss\n\n  @jax.jit\n  def loss_fun(params, l2reg, data, aux):\n    all_vars = {\"params\": params, \"batch_stats\": aux}\n    images, labels = data\n    logits, net_state = predict(all_vars, images, train=True)\n    loss = loss_from_logits(params, l2reg, logits, labels)\n    return loss, net_state[\"batch_stats\"]\n\n  def print_evaluation(params, state, l2reg, data, aux):\n    # We don't need `data` because we evaluate on the test set.\n    del data\n\n    if state.iter_num % FLAGS.evaluation_frequency == 0:\n      # Periodically evaluate classification accuracy on test set.\n      accuracy, loss = accuracy_and_loss(params, l2reg, data=next(test_ds),\n                                         aux=aux)\n\n      print(f\"[Step {state.iter_num}] \"\n            f\"Test accuracy: {accuracy:.3f} \"\n            f\"Test loss: {loss:.3f}.\")\n\n    return params, state\n\n  # Initialize solver and parameters.\n  solver = OptaxSolver(opt=optax.adam(FLAGS.learning_rate),\n                       fun=loss_fun,\n                       maxiter=FLAGS.maxiter,\n                       pre_update=print_evaluation,\n                       has_aux=True)\n\n  rng = jax.random.PRNGKey(0)\n  init_vars = net.init(rng, jnp.ones(input_shape), train=True)\n  params = init_vars[\"params\"]\n  batch_stats = init_vars[\"batch_stats\"]\n  state = solver.init_state(params)\n\n  for iternum in range(solver.maxiter):\n    params, state = solver.update(params=params, state=state,\n                                  l2reg=FLAGS.l2reg, data=next(train_ds),\n                                  aux=batch_stats)\n    batch_stats = state.aux\n\n\nif __name__ == \"__main__\":\n  app.run(main)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}