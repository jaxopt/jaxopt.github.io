{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# VAE example with Haiku and JAXopt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from typing import NamedTuple\n\nfrom absl import app\nfrom absl import flags\nimport haiku as hk\nimport jax\nimport jax.numpy as jnp\nfrom jaxopt import OptaxSolver\nimport numpy as onp\nimport optax\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\n\nMNIST_IMAGE_SHAPE = (28, 28, 1)\n\nflags.DEFINE_integer(\"batch_size\", 128, \"Size of the batch to train on.\")\nflags.DEFINE_float(\"learning_rate\", 0.001, \"Learning rate for the optimizer.\")\nflags.DEFINE_integer(\"training_steps\", 1000, \"Number of training steps to run.\")\nflags.DEFINE_integer(\"eval_frequency\", 100, \"How often to evaluate the model.\")\nflags.DEFINE_integer(\"random_seed\", 42, \"Random seed.\")\nFLAGS = flags.FLAGS\n\n\ndef load_dataset(split, batch_size):\n  ds = tfds.load(\"binarized_mnist\", split=split, shuffle_files=True,\n                 read_config=tfds.ReadConfig(shuffle_seed=FLAGS.random_seed))\n  ds = ds.shuffle(buffer_size=10 * batch_size, seed=FLAGS.random_seed)\n  ds = ds.batch(batch_size)\n  ds = ds.prefetch(buffer_size=5)\n  ds = ds.repeat()\n  return iter(tfds.as_numpy(ds))\n\n\nclass Encoder(hk.Module):\n  \"\"\"Encoder model.\"\"\"\n\n  def __init__(self, hidden_size=512, latent_size=10):\n    super().__init__()\n    self._hidden_size = hidden_size\n    self._latent_size = latent_size\n\n  def __call__(self, x):\n    x = hk.Flatten()(x)\n    x = hk.Linear(self._hidden_size)(x)\n    x = jax.nn.relu(x)\n\n    mean = hk.Linear(self._latent_size)(x)\n    log_stddev = hk.Linear(self._latent_size)(x)\n    stddev = jnp.exp(log_stddev)\n\n    return mean, stddev\n\n\nclass Decoder(hk.Module):\n  \"\"\"Decoder model.\"\"\"\n\n  def __init__(self, hidden_size=512, output_shape=MNIST_IMAGE_SHAPE):\n    super().__init__()\n    self._hidden_size = hidden_size\n    self._output_shape = output_shape\n\n  def __call__(self, z):\n    z = hk.Linear(self._hidden_size)(z)\n    z = jax.nn.relu(z)\n\n    logits = hk.Linear(onp.prod(self._output_shape))(z)\n    logits = jnp.reshape(logits, (-1, *self._output_shape))\n\n    return logits\n\n\nclass VAEOutput(NamedTuple):\n  image: jnp.ndarray\n  mean: jnp.ndarray\n  stddev: jnp.ndarray\n  logits: jnp.ndarray\n\n\nclass VariationalAutoEncoder(hk.Module):\n  \"\"\"Main VAE model class, uses Encoder & Decoder under the hood.\"\"\"\n\n  def __init__(self, hidden_size=512, latent_size=10,\n               output_shape=MNIST_IMAGE_SHAPE):\n    super().__init__()\n    self._hidden_size = hidden_size\n    self._latent_size = latent_size\n    self._output_shape = output_shape\n\n  def __call__(self, x):\n    x = x.astype(jnp.float32)\n    mean, stddev = Encoder(self._hidden_size, self._latent_size)(x)\n    z = mean + stddev * jax.random.normal(hk.next_rng_key(), mean.shape)\n    logits = Decoder(self._hidden_size, self._output_shape)(z)\n\n    p = jax.nn.sigmoid(logits)\n    image = jax.random.bernoulli(hk.next_rng_key(), p)\n\n    return VAEOutput(image, mean, stddev, logits)\n\n\ndef binary_cross_entropy(x: jnp.ndarray, logits: jnp.ndarray) -> jnp.ndarray:\n  if x.shape != logits.shape:\n    raise ValueError(\"inputs x and logits must be of the same shape\")\n\n  x = jnp.reshape(x, (x.shape[0], -1))\n  logits = jnp.reshape(logits, (logits.shape[0], -1))\n\n  return -jnp.sum(x * logits - jnp.logaddexp(0.0, logits), axis=-1)\n\n\ndef kl_gaussian(mean: jnp.ndarray, var: jnp.ndarray) -> jnp.ndarray:\n  r\"\"\"Calculate KL divergence between given and standard gaussian distributions.\n  KL(p, q) = H(p, q) - H(p) = -\\int p(x)log(q(x))dx - -\\int p(x)log(p(x))dx\n           = 0.5 * [log(|s2|/|s1|) - 1 + tr(s1/s2) + (m1-m2)^2/s2]\n           = 0.5 * [-log(|s1|) - 1 + tr(s1) + m1^2] (if m2 = 0, s2 = 1)\n  Args:\n    mean: mean vector of the first distribution\n    var: diagonal vector of covariance matrix of the first distribution\n  Returns:\n    A scalar representing KL divergence of the two Gaussian distributions.\n  \"\"\"\n  return 0.5 * jnp.sum(-jnp.log(var) - 1.0 + var + jnp.square(mean), axis=-1)\n\n\n# pylint: disable=unnecessary-lambda\nmodel = hk.transform(lambda x: VariationalAutoEncoder()(x))\n\n\n@jax.jit\ndef loss_fun(params, rng_key, batch):\n  \"\"\"ELBO loss: E_p[log(x)] - KL(d||q), where p ~ Be(0.5) and q ~ N(0,1).\"\"\"\n  outputs = model.apply(params, rng_key, batch[\"image\"])\n  log_likelihood = -binary_cross_entropy(batch[\"image\"], outputs.logits)\n  kl = kl_gaussian(outputs.mean, jnp.square(outputs.stddev))\n  elbo = log_likelihood - kl\n  return -jnp.mean(elbo)\n\n\ndef main(argv):\n  del argv\n\n  # Hide any GPUs from TensorFlow. Otherwise TF might reserve memory and make\n  # it unavailable to JAX.\n  tf.config.experimental.set_visible_devices([], 'GPU')\n\n  # Initialize solver.\n  solver = OptaxSolver(opt=optax.adam(FLAGS.learning_rate), fun=loss_fun)\n\n  # Set up data iterators.\n  train_ds = load_dataset(tfds.Split.TRAIN, FLAGS.batch_size)\n  test_ds = load_dataset(tfds.Split.TEST, FLAGS.batch_size)\n\n  # Initialize parameters.\n  rng_seq = hk.PRNGSequence(FLAGS.random_seed)\n  params = model.init(next(rng_seq), onp.zeros((1, *MNIST_IMAGE_SHAPE)))\n  state = solver.init_state(params)\n\n  # Run training loop.\n  for step in range(FLAGS.training_steps):\n    params, state = solver.update(params=params, state=state,\n                                  rng_key=next(rng_seq),\n                                  batch=next(train_ds))\n\n    if step % FLAGS.eval_frequency == 0:\n      val_loss = loss_fun(params, next(rng_seq), next(test_ds))\n      print(f\"STEP: {step}; Validation ELBO: {val_loss:.3f}\")\n\nif __name__ == \"__main__\":\n  app.run(main)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}