{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Implicit differentiation of lasso.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from absl import app\nfrom absl import flags\n\nimport jax\nimport jax.numpy as jnp\n\nfrom jaxopt import BlockCoordinateDescent\nfrom jaxopt import objective\nfrom jaxopt import OptaxSolver\nfrom jaxopt import prox\nfrom jaxopt import ProximalGradient\nimport optax\n\nfrom sklearn import datasets\nfrom sklearn import model_selection\nfrom sklearn import preprocessing\n\nflags.DEFINE_bool(\"unrolling\", False, \"Whether to use unrolling.\")\nflags.DEFINE_string(\"solver\", \"bcd\", \"Solver to use (bcd or pg).\")\nFLAGS = flags.FLAGS\n\n\ndef outer_objective(theta, init_inner, data):\n  \"\"\"Validation loss.\"\"\"\n  X_tr, X_val, y_tr, y_val = data\n  # We use the bijective mapping lam = jnp.exp(theta) to ensure positivity.\n  lam = jnp.exp(theta)\n\n  if FLAGS.solver == \"pg\":\n    solver = ProximalGradient(\n        fun=objective.least_squares,\n        prox=prox.prox_lasso,\n        implicit_diff=not FLAGS.unrolling,\n        maxiter=500)\n  elif FLAGS.solver == \"bcd\":\n    solver = BlockCoordinateDescent(\n        fun=objective.least_squares,\n        block_prox=prox.prox_lasso,\n        implicit_diff=not FLAGS.unrolling,\n        maxiter=500)\n  else:\n    raise ValueError(\"Unknown solver.\")\n\n  # The format is run(init_params, hyperparams_prox, *args, **kwargs)\n  # where *args and **kwargs are passed to `fun`.\n  w_fit = solver.run(init_inner, lam, (X_tr, y_tr)).params\n\n  y_pred = jnp.dot(X_val, w_fit)\n  loss_value = jnp.mean((y_pred - y_val) ** 2)\n\n  # We return w_fit as auxiliary data.\n  # Auxiliary data is stored in the optimizer state (see below).\n  return loss_value, w_fit\n\n\ndef main(argv):\n  del argv\n\n  print(\"Solver:\", FLAGS.solver)\n  print(\"Unrolling:\", FLAGS.unrolling)\n\n  # Prepare data.\n  X, y = datasets.load_boston(return_X_y=True)\n  X = preprocessing.normalize(X)\n  # data = (X_tr, X_val, y_tr, y_val)\n  data = model_selection.train_test_split(X, y, test_size=0.33, random_state=0)\n\n  # Initialize solver.\n  solver = OptaxSolver(opt=optax.adam(1e-2), fun=outer_objective, has_aux=True)\n  theta = 1.0\n  init_w = jnp.zeros(X.shape[1])\n  state = solver.init_state(theta, init_inner=init_w, data=data)\n\n  # Run outer loop.\n  for _ in range(10):\n    theta, state = solver.update(params=theta, state=state, init_inner=init_w,\n                                 data=data)\n    # The auxiliary data returned by the outer loss is stored in the state.\n    init_w = state.aux\n    print(f\"[Step {state.iter_num}] Validation loss: {state.value:.3f}.\")\n\nif __name__ == \"__main__\":\n  app.run(main)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}