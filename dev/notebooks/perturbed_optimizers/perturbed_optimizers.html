

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Perturbed optimizers &mdash; JAXopt 0.8.3 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/plot_directive.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css?v=d2d258e8" />
      <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css?v=f4aeca0c" />
      <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
      <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css?v=37e83ca3" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=7f96dc5e"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=f281be69"></script>
      <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            JAXopt
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../basics.html">Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../unconstrained.html">Unconstrained optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../constrained.html">Constrained optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quadratic_programming.html">Quadratic programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../non_smooth.html">Non-smooth optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../stochastic.html">Stochastic optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../root_finding.html">Root finding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fixed_point.html">Fixed point resolution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nonlinear_least_squares.html">Nonlinear least squares</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../linear_system_solvers.html">Linear system solving</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../implicit_diff.html">Implicit differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../objective_and_loss.html">Loss and objective functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../line_search.html">Line search</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../perturbations.html">Perturbed optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api.html">API at a glance</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../index.html">Notebook gallery</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../auto_examples/index.html">Example gallery</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">About</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://github.com/google/jaxopt/graphs/contributors">Authors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/google/jaxopt">Source code</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/google/jaxopt/issues">Issue tracker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developer.html">Development</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">JAXopt</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Perturbed optimizers</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/google/jaxopt/blob/main/docs/notebooks/perturbed_optimizers/perturbed_optimizers.ipynb" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <p>Copyright 2022 Google LLC</p>
<p>Licensed under the Apache License, Version 2.0 (the “License”); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p>
<p>https://www.apache.org/licenses/LICENSE-2.0</p>
<p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>
<section class="tex2jax_ignore mathjax_ignore" id="perturbed-optimizers">
<h1>Perturbed optimizers<a class="headerlink" href="#perturbed-optimizers" title="Link to this heading"></a></h1>
<p><a class="reference external" href="https://colab.research.google.com/github/google/jaxopt/blob/main/docs/notebooks/perturbed_optimizers/perturbed_optimizers.ipynb"><img alt="Open in Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<p>We review in this notebook a universal method to transform any optimizer <span class="math notranslate nohighlight">\(y^*\)</span> in a differentiable approximation <span class="math notranslate nohighlight">\(y_\varepsilon^*\)</span>, using pertutbations following the method of <a class="reference external" href="https://arxiv.org/abs/2002.08676">Berthet et al. (2020)</a>. JAXopt provides an implementation that we illustrate here on some examples.</p>
<p>Concretely, for an optimizer function <span class="math notranslate nohighlight">\(y^*\)</span> defined by</p>
<div class="math notranslate nohighlight">
\[y^*(\theta) = \mathop{\mathrm{arg\,max}}_{y\in \mathcal{C}} \langle y, \theta \rangle\, ,\]</div>
<p>we consider, for a random <span class="math notranslate nohighlight">\(Z\)</span> drawn from a distribution with continuous positive distribution <span class="math notranslate nohighlight">\(\mu\)</span></p>
<div class="math notranslate nohighlight">
\[y_\varepsilon^*(\theta) = \mathbf{E}[\mathop{\mathrm{arg\,max}}_{y\in \mathcal{C}} \langle y, \theta + \varepsilon Z \rangle]\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%capture</span>
<span class="o">%</span><span class="n">pip</span> <span class="n">install</span> <span class="n">jaxopt</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># activate TPUs if available</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">jax.tools.colab_tpu</span>
    <span class="n">jax</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">colab_tpu</span><span class="o">.</span><span class="n">setup_tpu</span><span class="p">()</span>
<span class="k">except</span> <span class="p">(</span><span class="ne">KeyError</span><span class="p">,</span> <span class="ne">RuntimeError</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TPU not found, continuing without it.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TPU not found, continuing without it.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">jax</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">jaxopt</span><span class="w"> </span><span class="kn">import</span> <span class="n">perturbations</span>
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="argmax-one-hot">
<h1>Argmax one-hot<a class="headerlink" href="#argmax-one-hot" title="Link to this heading"></a></h1>
<p>We consider an optimizer, such as the following <code class="docutils literal notranslate"><span class="pre">argmax_one_hot</span></code> function. It transforms a real-valued vector into a binary vector with a 1 in the coefficient with largest magnitude and 0 elsewhere. It corresponds to <span class="math notranslate nohighlight">\(y^*\)</span> for <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> being the unit simplex. We run it on an example input <code class="docutils literal notranslate"><span class="pre">values</span></code>.</p>
<section id="one-hot-function">
<h2>One-hot function<a class="headerlink" href="#one-hot-function" title="Link to this heading"></a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">argmax_one_hot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">),</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">values</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">1.9</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">])</span>

<span class="n">one_hot_vec</span> <span class="o">=</span> <span class="n">argmax_one_hot</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">one_hot_vec</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0. 1. 0. 0. 0.]
</pre></div>
</div>
</div>
</div>
</section>
<section id="one-hot-with-pertubations">
<h2>One-hot with pertubations<a class="headerlink" href="#one-hot-with-pertubations" title="Link to this heading"></a></h2>
<p>Our implementation transforms the <code class="docutils literal notranslate"><span class="pre">argmax_one_hot</span></code> function into a perturbed one that we call <code class="docutils literal notranslate"><span class="pre">pert_one_hot</span></code>. In this case we use Gumbel noise for the perturbation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">N_SAMPLES</span> <span class="o">=</span> <span class="mi">100_000</span>
<span class="n">SIGMA</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">GUMBEL</span> <span class="o">=</span> <span class="n">perturbations</span><span class="o">.</span><span class="n">Gumbel</span><span class="p">()</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">pert_one_hot</span> <span class="o">=</span> <span class="n">perturbations</span><span class="o">.</span><span class="n">make_perturbed_argmax</span><span class="p">(</span><span class="n">argmax_fun</span><span class="o">=</span><span class="n">argmax_one_hot</span><span class="p">,</span>
                                         <span class="n">num_samples</span><span class="o">=</span><span class="n">N_SAMPLES</span><span class="p">,</span>
                                         <span class="n">sigma</span><span class="o">=</span><span class="n">SIGMA</span><span class="p">,</span>
                                         <span class="n">noise</span><span class="o">=</span><span class="n">GUMBEL</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>In this particular case, it is equal to the usual <a class="reference external" href="https://en.wikipedia.org/wiki/Softmax_function">softmax function</a>. This is not always true, in general there is no closed form for <span class="math notranslate nohighlight">\(y_\varepsilon^*\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rngs</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">rngs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">pert_argmax</span> <span class="o">=</span> <span class="n">pert_one_hot</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;computation with </span><span class="si">{</span><span class="n">N_SAMPLES</span><span class="si">}</span><span class="s1"> samples, sigma = </span><span class="si">{</span><span class="n">SIGMA</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;perturbed argmax = </span><span class="si">{</span><span class="n">pert_argmax</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">values</span><span class="o">/</span><span class="n">SIGMA</span><span class="p">)</span>
<span class="n">soft_max</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">values</span><span class="o">/</span><span class="n">SIGMA</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;softmax = </span><span class="si">{</span><span class="n">soft_max</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;square norm of softmax = </span><span class="si">{</span><span class="n">jnp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">soft_max</span><span class="p">)</span><span class="si">:</span><span class="s1">.2e</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;square norm of difference = </span><span class="si">{</span><span class="n">jnp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">pert_argmax</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">soft_max</span><span class="p">)</span><span class="si">:</span><span class="s1">.2e</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>computation with 100000 samples, sigma = 0.5
perturbed argmax = [0.00533 0.81592 0.01209 0.16403 0.00263]
softmax = [0.00549293 0.8152234  0.01222475 0.16459078 0.00246813]
square norm of softmax = 8.32e-01
square norm of difference = 9.33e-04
</pre></div>
</div>
</div>
</div>
</section>
<section id="gradients-for-one-hot-with-perturbations">
<h2>Gradients for one-hot with perturbations<a class="headerlink" href="#gradients-for-one-hot-with-perturbations" title="Link to this heading"></a></h2>
<p>The perturbed optimizer <span class="math notranslate nohighlight">\(y_\varepsilon^*\)</span> is differentiable, and its gradient can be computed with stochastic estimation automatically, using <code class="docutils literal notranslate"><span class="pre">jax.grad</span></code>.</p>
<p>We create a scalar loss <code class="docutils literal notranslate"><span class="pre">loss_simplex</span></code> of the perturbed optimizer <span class="math notranslate nohighlight">\(y^*_\varepsilon\)</span></p>
<div class="math notranslate nohighlight">
\[\ell_\text{simplex}(y_{\text{true}} = y_\varepsilon^*; y_{\text{true}})\]</div>
<p>For <code class="docutils literal notranslate"><span class="pre">values</span></code> equal to a vector <span class="math notranslate nohighlight">\(\theta\)</span>, we can compute gradients of</p>
<div class="math notranslate nohighlight">
\[\ell(\theta) = \ell_\text{simplex}(y_\varepsilon^*(\theta); y_{\text{true}})\]</div>
<p>with respect to <code class="docutils literal notranslate"><span class="pre">values</span></code>, automatically.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example loss function</span>

<span class="k">def</span><span class="w"> </span><span class="nf">loss_simplex</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">rng</span><span class="p">):</span>
  <span class="n">n</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">v_true</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span>
  <span class="n">y_true</span> <span class="o">=</span> <span class="n">v_true</span> <span class="o">/</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">v_true</span><span class="p">)</span>
  <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pert_one_hot</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">loss_simplex</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">rngs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Array(0.5832315, dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>We can compute the gradient of <span class="math notranslate nohighlight">\(\ell\)</span> directly</p>
<div class="math notranslate nohighlight">
\[\nabla_\theta \ell(\theta) = \partial_\theta y^*_\varepsilon(\theta) \cdot \nabla_1 \ell_{\text{simplex}}(y^*_\varepsilon(\theta); y_{\text{true}})\]</div>
<p>The computation of the jacobian <span class="math notranslate nohighlight">\(\partial_\theta y^*_\varepsilon(\theta)\)</span> is implemented automatically, using an estimation method given by <a class="reference external" href="https://arxiv.org/abs/2002.08676">Berthet et al. (2020)</a>, [Prop. 3.1].</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Gradient of the loss w.r.t input values</span>

<span class="n">gradient</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss_simplex</span><span class="p">)(</span><span class="n">values</span><span class="p">,</span> <span class="n">rngs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">gradient</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[-0.00709645  0.4648722  -0.03839324 -0.4134317  -0.00294876]
</pre></div>
</div>
</div>
</div>
<p>We illustrate the use of this method by running 200 steps of gradient descent on <span class="math notranslate nohighlight">\(\theta_t\)</span> so that it minimizes this loss.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Doing 200 steps of gradient descent on the values to have the desired ranks</span>

<span class="n">steps</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">values_t</span> <span class="o">=</span> <span class="n">values</span>
<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.5</span>

<span class="n">grad_func</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss_simplex</span><span class="p">))</span>

<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
  <span class="n">rngs</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rngs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>
  <span class="n">values_t</span> <span class="o">=</span> <span class="n">values_t</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">grad_func</span><span class="p">(</span><span class="n">values_t</span><span class="p">,</span> <span class="n">rngs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rngs</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rngs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">n</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">v_true</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">v_true</span> <span class="o">/</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">v_true</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;initial values = </span><span class="si">{</span><span class="n">values</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;initial one-hot = </span><span class="si">{</span><span class="n">argmax_one_hot</span><span class="p">(</span><span class="n">values</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;initial diff. one-hot = </span><span class="si">{</span><span class="n">pert_one_hot</span><span class="p">(</span><span class="n">values</span><span class="p">,</span><span class="w"> </span><span class="n">rngs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;values after GD = </span><span class="si">{</span><span class="n">values_t</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;ranks after GD = </span><span class="si">{</span><span class="n">argmax_one_hot</span><span class="p">(</span><span class="n">values_t</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;diff. one-hot after GD = </span><span class="si">{</span><span class="n">pert_one_hot</span><span class="p">(</span><span class="n">values_t</span><span class="p">,</span><span class="w"> </span><span class="n">rngs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;target diff. one-hot = </span><span class="si">{</span><span class="n">y_true</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>initial values = [-0.6  1.9 -0.2  1.1 -1. ]
initial one-hot = [0. 1. 0. 0. 0.]
initial diff. one-hot = [0.00522    0.81456995 0.01213    0.16549    0.00259   ]

values after GD = [-0.06480958  0.13771233  0.28429762  0.39511892  0.4852356 ]
ranks after GD = [0. 0. 0. 0. 1.]
diff. one-hot after GD = [0.10061999 0.15027    0.20278    0.24774    0.29859   ]
target diff. one-hot = [0.1  0.15 0.2  0.25 0.3 ]
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="differentiable-ranking">
<h1>Differentiable ranking<a class="headerlink" href="#differentiable-ranking" title="Link to this heading"></a></h1>
<section id="ranking-function">
<h2>Ranking function<a class="headerlink" href="#ranking-function" title="Link to this heading"></a></h2>
<p>We consider an optimizer, such as the following <code class="docutils literal notranslate"><span class="pre">ranking</span></code> function. It transforms a real-valued vector of size <span class="math notranslate nohighlight">\(n\)</span> into a vector with coefficients being a permutation of <span class="math notranslate nohighlight">\(\{0,\ldots, n-1\}\)</span> corresponding to the order of the coefficients of the original vector. It corresponds to <span class="math notranslate nohighlight">\(y^*\)</span> for <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> being the permutahedron. We run it on an example input <code class="docutils literal notranslate"><span class="pre">values</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Function outputting a vector of ranks</span>

<span class="k">def</span><span class="w"> </span><span class="nf">ranking</span><span class="p">(</span><span class="n">values</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">values</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example on random values</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">6</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">values</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="p">(</span><span class="n">n</span><span class="p">,))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;values = </span><span class="si">{</span><span class="n">values</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;ranking = </span><span class="si">{</span><span class="n">ranking</span><span class="p">(</span><span class="n">values</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>values = [ 1.6226422   2.0252647  -0.43359444 -0.07861735  0.1760909  -0.97208923]
ranking = [4 5 1 2 3 0]
</pre></div>
</div>
</div>
</div>
</section>
<section id="ranking-with-perturbations">
<h2>Ranking with perturbations<a class="headerlink" href="#ranking-with-perturbations" title="Link to this heading"></a></h2>
<p>As above, our implementation transforms this function into a perturbed one that we call <code class="docutils literal notranslate"><span class="pre">pert_ranking</span></code>. In this case we use Gumbel noise for the perturbation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">N_SAMPLES</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">SIGMA</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">GUMBEL</span> <span class="o">=</span> <span class="n">perturbations</span><span class="o">.</span><span class="n">Gumbel</span><span class="p">()</span>

<span class="n">pert_ranking</span> <span class="o">=</span> <span class="n">perturbations</span><span class="o">.</span><span class="n">make_perturbed_argmax</span><span class="p">(</span><span class="n">ranking</span><span class="p">,</span>
                                                   <span class="n">num_samples</span><span class="o">=</span><span class="n">N_SAMPLES</span><span class="p">,</span>
                                                   <span class="n">sigma</span><span class="o">=</span><span class="n">SIGMA</span><span class="p">,</span>
                                                   <span class="n">noise</span><span class="o">=</span><span class="n">GUMBEL</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Expectation of the perturbed ranks on these values</span>

<span class="n">rngs</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">diff_ranks</span> <span class="o">=</span> <span class="n">pert_ranking</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">rngs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;values = </span><span class="si">{</span><span class="n">values</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;diff_ranks = </span><span class="si">{</span><span class="n">diff_ranks</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>values = [ 1.6226422   2.0252647  -0.43359444 -0.07861735  0.1760909  -0.97208923]
diff_ranks = [4.11      4.89      1.1899999 2.07      2.7       0.04     ]
</pre></div>
</div>
</div>
</div>
</section>
<section id="gradients-for-ranking-with-perturbations">
<h2>Gradients for ranking with perturbations<a class="headerlink" href="#gradients-for-ranking-with-perturbations" title="Link to this heading"></a></h2>
<p>As above, the perturbed optimizer <span class="math notranslate nohighlight">\(y_\varepsilon^*\)</span> is differentiable, and its gradient can be computed with stochastic estimation automatically, using <code class="docutils literal notranslate"><span class="pre">jax.grad</span></code>.</p>
<p>We showcase this on a loss of <span class="math notranslate nohighlight">\(y_\varepsilon(\theta)\)</span> that can be directly differentiated w.r.t. the <code class="docutils literal notranslate"><span class="pre">values</span></code> equal to <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example loss function</span>

<span class="k">def</span><span class="w"> </span><span class="nf">loss_example</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">rng</span><span class="p">):</span>
  <span class="n">n</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">y_true</span> <span class="o">=</span> <span class="n">ranking</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
  <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pert_ranking</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">loss_example</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">rngs</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>59.820396
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Gradient of the objective w.r.t input values</span>

<span class="n">gradient</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss_example</span><span class="p">)(</span><span class="n">values</span><span class="p">,</span> <span class="n">rngs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">gradient</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[-109.08817    -26.930592    -1.7325559   17.05715    -50.017895
   10.387624 ]
</pre></div>
</div>
</div>
</div>
<p>As above, we showcase this example on gradient descent to minimize this loss.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">steps</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">values_t</span> <span class="o">=</span> <span class="n">values</span>
<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="n">grad_func</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss_example</span><span class="p">))</span>

<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
  <span class="n">rngs</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rngs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>
  <span class="n">values_t</span> <span class="o">=</span> <span class="n">values_t</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">grad_func</span><span class="p">(</span><span class="n">values_t</span><span class="p">,</span> <span class="n">rngs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rngs</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rngs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">y_true</span> <span class="o">=</span> <span class="n">ranking</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;initial values = </span><span class="si">{</span><span class="n">values</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;initial ranks = </span><span class="si">{</span><span class="n">ranking</span><span class="p">(</span><span class="n">values</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;initial diff. ranks = </span><span class="si">{</span><span class="n">pert_ranking</span><span class="p">(</span><span class="n">values</span><span class="p">,</span><span class="w"> </span><span class="n">rngs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;values after GD = </span><span class="si">{</span><span class="n">values_t</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;ranks after GD = </span><span class="si">{</span><span class="n">ranking</span><span class="p">(</span><span class="n">values_t</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;diff. ranks after GD = </span><span class="si">{</span><span class="n">pert_ranking</span><span class="p">(</span><span class="n">values_t</span><span class="p">,</span><span class="w"> </span><span class="n">rngs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;target diff. ranks = </span><span class="si">{</span><span class="n">y_true</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>initial values = [ 1.6226422   2.0252647  -0.43359444 -0.07861735  0.1760909  -0.97208923]
initial ranks = [4 5 1 2 3 0]
initial diff. ranks = [4.14       4.8599997  1.01       2.12       2.72       0.14999999]

values after GD = [-5.5964713  -0.04083824 -1.4046352  -0.17436153  4.953107    2.9552662 ]
ranks after GD = [0 3 1 2 5 4]
diff. ranks after GD = [0.        2.6299999 1.        2.37      5.        4.       ]
target diff. ranks = [0 1 2 3 4 5]
</pre></div>
</div>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021-2022, the JAXopt authors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>