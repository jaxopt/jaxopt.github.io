<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>jaxopt._src.lbfgsb &mdash; JAXopt 0.8 documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/plot_directive.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/sg_gallery-rendered-html.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/sphinx_highlight.js"></script>
        <script src="../../../_static/clipboard.min.js"></script>
        <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            JAXopt
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../basics.html">Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../unconstrained.html">Unconstrained optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../constrained.html">Constrained optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../quadratic_programming.html">Quadratic programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../non_smooth.html">Non-smooth optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../stochastic.html">Stochastic optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../root_finding.html">Root finding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fixed_point.html">Fixed point resolution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nonlinear_least_squares.html">Nonlinear least squares</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../linear_system_solvers.html">Linear system solving</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../implicit_diff.html">Implicit differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../objective_and_loss.html">Loss and objective functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../line_search.html">Line search</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../perturbations.html">Perturbed optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api.html">API at a glance</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/index.html">Notebook gallery</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../auto_examples/index.html">Example gallery</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">About</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://github.com/google/jaxopt/graphs/contributors">Authors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/google/jaxopt">Source code</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/google/jaxopt/issues">Issue tracker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../developer.html">Development</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">JAXopt</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">jaxopt._src.lbfgsb</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for jaxopt._src.lbfgsb</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2023 Google LLC</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>

<span class="sd">&quot;&quot;&quot;Limited-memory BFGS with box constraints.&quot;&quot;&quot;</span>

<span class="c1"># This is based on:</span>
<span class="c1"># [1] R. H. Byrd, P. Lu and J. Nocedal. A Limited Memory Algorithm for Bound</span>
<span class="c1"># Constrained Optimization, (1995), SIAM Journal on Scientific and Statistical</span>
<span class="c1"># Computing, 16, 5, pp. 1190-1208.</span>
<span class="c1"># [2] J. Nocedal and S. Wright.  Numerical Optimization, second edition.</span>

<span class="kn">import</span> <span class="nn">dataclasses</span>
<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">NamedTuple</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">jnp</span>

<span class="kn">from</span> <span class="nn">jaxopt._src</span> <span class="kn">import</span> <span class="n">base</span>
<span class="kn">from</span> <span class="nn">jaxopt._src</span> <span class="kn">import</span> <span class="n">projection</span>
<span class="kn">from</span> <span class="nn">jaxopt._src.lbfgs</span> <span class="kn">import</span> <span class="n">init_history</span>
<span class="kn">from</span> <span class="nn">jaxopt._src.lbfgs</span> <span class="kn">import</span> <span class="n">update_history</span>
<span class="kn">from</span> <span class="nn">jaxopt._src.linesearch_util</span> <span class="kn">import</span> <span class="n">_init_stepsize</span>
<span class="kn">from</span> <span class="nn">jaxopt._src.linesearch_util</span> <span class="kn">import</span> <span class="n">_setup_linesearch</span>

<span class="kn">from</span> <span class="nn">jaxopt._src.tree_util</span> <span class="kn">import</span> <span class="n">tree_single_dtype</span>
<span class="kn">from</span> <span class="nn">jaxopt.tree_util</span> <span class="kn">import</span> <span class="n">tree_add_scalar_mul</span>
<span class="kn">from</span> <span class="nn">jaxopt.tree_util</span> <span class="kn">import</span> <span class="n">tree_inf_norm</span>
<span class="kn">from</span> <span class="nn">jaxopt.tree_util</span> <span class="kn">import</span> <span class="n">tree_map</span>
<span class="kn">from</span> <span class="nn">jaxopt.tree_util</span> <span class="kn">import</span> <span class="n">tree_sub</span>
<span class="kn">from</span> <span class="nn">jaxopt.tree_util</span> <span class="kn">import</span> <span class="n">tree_vdot</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="k">def</span> <span class="nf">_flatten_and_concat</span><span class="p">(</span><span class="n">tree</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">batch_ndims</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Flattens a pytree and concatenates leaves along the last dimension.&quot;&quot;&quot;</span>
  <span class="n">r</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_util</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span>
      <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="n">batch_ndims</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)),</span> <span class="n">tree</span>
  <span class="p">)</span>
  <span class="k">return</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_util</span><span class="o">.</span><span class="n">tree_reduce</span><span class="p">(</span>
      <span class="k">lambda</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">r</span>
  <span class="p">)</span>


<span class="k">def</span> <span class="nf">_split_and_pack_like</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">tree</span><span class="p">:</span> <span class="n">Any</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Splits an array and packs the components like the provided pytree.&quot;&quot;&quot;</span>
  <span class="n">treedef</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_util</span><span class="o">.</span><span class="n">tree_structure</span><span class="p">(</span><span class="n">tree</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_util</span><span class="o">.</span><span class="n">treedef_is_leaf</span><span class="p">(</span><span class="n">treedef</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tree</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
  <span class="n">sizes</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_util</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">tree</span><span class="p">)</span>
  <span class="n">flat_sizes</span><span class="p">,</span> <span class="n">treedef</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_util</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">sizes</span><span class="p">)</span>
  <span class="n">splits</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">flat_sizes</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
  <span class="k">return</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_util</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">y</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span>
                                <span class="n">jax</span><span class="o">.</span><span class="n">tree_util</span><span class="o">.</span><span class="n">tree_unflatten</span><span class="p">(</span><span class="n">treedef</span><span class="p">,</span> <span class="n">splits</span><span class="p">),</span>
                                <span class="n">tree</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_error</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">grad</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">lower</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">upper</span><span class="p">:</span> <span class="n">Any</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Computes the error for determining convergence.&quot;&quot;&quot;</span>
  <span class="n">err</span> <span class="o">=</span> <span class="n">tree_map</span><span class="p">(</span>
      <span class="k">lambda</span> <span class="n">y</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">u</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">g</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">u</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span>
  <span class="p">)</span>
  <span class="k">return</span> <span class="n">tree_inf_norm</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_find_cauchy_point</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">grad</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">lower</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">upper</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">w</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">m</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Finds the Cauchy point.</span>

<span class="sd">  The Cauchy point is the first local minimizer of the quadratic model along the</span>
<span class="sd">  piecewise linear path obtained by projecting points along the steepest descent</span>
<span class="sd">  direction. This function implements Algorithm CP in [1].</span>

<span class="sd">  Args:</span>
<span class="sd">    x: Parameters.</span>
<span class="sd">    grad: Gradients with respect to parameters.</span>
<span class="sd">    lower: Parameter lower bounds.</span>
<span class="sd">    upper: Parameter upper bounds.</span>
<span class="sd">    theta: Scaling parameter on the identity matrix for the initial Hessian</span>
<span class="sd">      approximation.</span>
<span class="sd">    w: `W_k` matrix in equation 3.3 of [1], computed from the correction</span>
<span class="sd">      matrices.</span>
<span class="sd">    m: `M_k` matrix in equation 3.4 of [1], computed from the correction</span>
<span class="sd">      matrices.</span>

<span class="sd">  Returns:</span>
<span class="sd">    x_cauchy: The Cauchy point.</span>
<span class="sd">    c: Vector to initialize subspace minimization.</span>
<span class="sd">    active_set_mask: Boolean mask where `True` indicates that the coordinate is</span>
<span class="sd">      in the active set (not equal to the lower or upper bound).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># TODO(emilyaf, srvasude): Consider a cheaper Cauchy point approximation:</span>
  <span class="c1"># https://yunfei.work/lbfgsb/lbfgsb_tech_report.pdf</span>
  <span class="n">eps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>
  <span class="n">t</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
      <span class="n">jnp</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span>
      <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">grad</span> <span class="o">&lt;</span> <span class="mf">0.</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">upper</span><span class="p">)</span> <span class="o">/</span> <span class="n">grad</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">lower</span><span class="p">)</span> <span class="o">/</span> <span class="n">grad</span><span class="p">))</span>
  <span class="n">d</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">t</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="o">-</span><span class="n">grad</span><span class="p">)</span>
  <span class="n">x_bound</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">d</span> <span class="o">&gt;</span> <span class="mf">0.</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">d</span> <span class="o">&lt;</span> <span class="mf">0.</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>

  <span class="c1"># Sort coordinates by the distance from the bounds, divided by the gradient.</span>
  <span class="n">t_ind</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">t_sorted</span> <span class="o">=</span> <span class="n">t</span><span class="p">[</span><span class="n">t_ind</span><span class="p">]</span>
  <span class="n">dt</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">t_sorted</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="s2">&quot;constant&quot;</span><span class="p">))</span>

  <span class="c1"># Begin the loop at the first coordinate that is not at a bound.</span>
  <span class="n">active_set_mask_sorted</span> <span class="o">=</span> <span class="n">t_sorted</span> <span class="o">&gt;</span> <span class="n">eps</span>
  <span class="n">start_ind</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span>
      <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">active_set_mask_sorted</span><span class="p">,</span>
                       <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">bool_</span><span class="p">)],</span>
                      <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

  <span class="n">init_c</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
  <span class="n">init_p</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
  <span class="n">init_df</span> <span class="o">=</span> <span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
  <span class="n">init_ddf</span> <span class="o">=</span> <span class="o">-</span><span class="n">theta</span> <span class="o">*</span> <span class="n">init_df</span> <span class="o">-</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">init_p</span><span class="p">),</span> <span class="n">init_p</span><span class="p">)</span>
  <span class="n">init_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">start_ind</span><span class="p">,</span> <span class="n">init_df</span><span class="p">,</span> <span class="n">init_ddf</span><span class="p">,</span> <span class="n">active_set_mask_sorted</span><span class="p">,</span> <span class="n">init_c</span><span class="p">,</span>
                <span class="n">init_p</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_cond</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="o">-</span><span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">state</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">dt</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

  <span class="k">def</span> <span class="nf">_body</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
    <span class="n">i</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">ddf</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">args</span>
    <span class="n">j</span> <span class="o">=</span> <span class="n">t_ind</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>  <span class="c1"># index of the unsorted array</span>
    <span class="n">c_new</span> <span class="o">=</span> <span class="n">c</span> <span class="o">+</span> <span class="n">dt</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">p</span>
    <span class="n">df_new</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">df</span>
        <span class="o">+</span> <span class="n">dt</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">ddf</span>
        <span class="o">+</span> <span class="n">grad</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="o">+</span> <span class="n">theta</span> <span class="o">*</span> <span class="n">grad</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_bound</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
        <span class="o">-</span> <span class="n">grad</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">jnp</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">c_new</span><span class="p">))</span>
    <span class="p">)</span>
    <span class="n">ddf_new</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">ddf</span>
        <span class="o">-</span> <span class="n">theta</span> <span class="o">*</span> <span class="n">grad</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="o">-</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">grad</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">jnp</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
        <span class="o">-</span> <span class="n">grad</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">jnp</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">w</span><span class="p">[</span><span class="n">j</span><span class="p">]))</span>
    <span class="p">)</span>
    <span class="n">ddf_new</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">eps</span><span class="p">,</span> <span class="n">ddf_new</span><span class="p">)</span>
    <span class="n">p_new</span> <span class="o">=</span> <span class="n">p</span> <span class="o">+</span> <span class="n">grad</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">w</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">df_new</span><span class="p">,</span> <span class="n">ddf_new</span><span class="p">,</span> <span class="n">mask</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="kc">False</span><span class="p">),</span> <span class="n">c_new</span><span class="p">,</span> <span class="n">p_new</span><span class="p">)</span>

  <span class="n">i</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">ddf</span><span class="p">,</span> <span class="n">mask_sorted</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">while_loop</span><span class="p">(</span><span class="n">_cond</span><span class="p">,</span> <span class="n">_body</span><span class="p">,</span> <span class="n">init_state</span><span class="p">)</span>
  <span class="n">dt_min</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="o">-</span><span class="n">df</span> <span class="o">/</span> <span class="n">ddf</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
  <span class="n">dt_min</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">dt_min</span><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">dt_min</span><span class="p">)</span>
  <span class="n">t_old</span> <span class="o">=</span> <span class="p">(</span>
      <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span>
          <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">t_sorted</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="p">)</span>
      <span class="o">+</span> <span class="n">dt_min</span>
  <span class="p">)</span>
  <span class="n">active_set_mask</span> <span class="o">=</span> <span class="n">mask_sorted</span><span class="p">[</span><span class="n">jnp</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">t_ind</span><span class="p">)]</span>
  <span class="n">x_cauchy</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">active_set_mask</span><span class="p">,</span> <span class="n">x</span> <span class="o">+</span> <span class="n">t_old</span> <span class="o">*</span> <span class="n">d</span><span class="p">,</span> <span class="n">x_bound</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">x_cauchy</span><span class="p">,</span> <span class="n">c</span> <span class="o">+</span> <span class="n">dt_min</span> <span class="o">*</span> <span class="n">p</span><span class="p">,</span> <span class="n">active_set_mask</span>


<span class="k">def</span> <span class="nf">_minimize_subspace</span><span class="p">(</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="n">x_cauchy</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">active_set_mask</span>
<span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Direct primal method of subspace minimization from [1].&quot;&quot;&quot;</span>
  <span class="n">w_masked</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">active_set_mask</span><span class="p">[:,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
  <span class="n">r_c</span> <span class="o">=</span> <span class="p">(</span>
      <span class="n">grad</span> <span class="o">+</span> <span class="n">theta</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_cauchy</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w_masked</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">c</span><span class="p">))</span>
  <span class="p">)</span>  <span class="c1"># eq. 5.4</span>

  <span class="c1"># TODO(emilyaf): Implement the method from [1] for a large number of variables</span>
  <span class="c1"># and few active constraints.</span>
  <span class="n">v</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w_masked</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">r_c</span><span class="p">)</span>
  <span class="n">v</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
  <span class="n">n</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">w_masked</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">w_masked</span><span class="p">)</span> <span class="o">/</span> <span class="n">theta</span>
  <span class="n">n</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">w</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">-</span> <span class="n">jnp</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
  <span class="n">v</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
  <span class="n">du</span> <span class="o">=</span> <span class="o">-</span><span class="n">r_c</span> <span class="o">/</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">jnp</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">w_masked</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="o">/</span> <span class="n">theta</span><span class="o">**</span><span class="mi">2</span>

  <span class="c1"># TODO(emilyaf, srvasude): Investigate whether to instead truncate</span>
  <span class="c1"># `x_cauchy + alpha_star * du` at the boundary, following</span>
  <span class="c1"># https://dl.acm.org/doi/abs/10.1145/2049662.2049669.</span>
  <span class="n">alpha</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">maximum</span><span class="p">((</span><span class="n">upper</span> <span class="o">-</span> <span class="n">x_cauchy</span><span class="p">)</span> <span class="o">/</span> <span class="n">du</span><span class="p">,</span> <span class="p">(</span><span class="n">lower</span> <span class="o">-</span> <span class="n">x_cauchy</span><span class="p">)</span> <span class="o">/</span> <span class="n">du</span><span class="p">)</span>
  <span class="n">alpha</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">active_set_mask</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">du</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.</span><span class="p">),</span> <span class="n">alpha</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span>
  <span class="n">alpha_star</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">alpha_star</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">alpha_star</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">active_set_mask</span><span class="p">,</span> <span class="n">x_cauchy</span> <span class="o">+</span> <span class="n">alpha_star</span> <span class="o">*</span> <span class="n">du</span><span class="p">,</span> <span class="n">x_cauchy</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">LbfgsbState</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Named tuple containing state information.&quot;&quot;&quot;</span>

  <span class="n">iter_num</span><span class="p">:</span> <span class="nb">int</span>
  <span class="n">value</span><span class="p">:</span> <span class="nb">float</span>
  <span class="n">grad</span><span class="p">:</span> <span class="n">Any</span>
  <span class="n">stepsize</span><span class="p">:</span> <span class="nb">float</span>
  <span class="n">error</span><span class="p">:</span> <span class="nb">float</span>
  <span class="n">s_history</span><span class="p">:</span> <span class="n">Any</span>
  <span class="n">y_history</span><span class="p">:</span> <span class="n">Any</span>
  <span class="n">theta</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span>
  <span class="n">num_updates</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ndarray</span>
  <span class="n">aux</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="n">failed_linesearch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>

  <span class="n">num_fun_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">num_grad_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">num_linesearch_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>


<div class="viewcode-block" id="LBFGSB"><a class="viewcode-back" href="../../../_autosummary/jaxopt.LBFGSB.html#jaxopt.LBFGSB">[docs]</a><span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span><span class="p">(</span><span class="n">eq</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">LBFGSB</span><span class="p">(</span><span class="n">base</span><span class="o">.</span><span class="n">IterativeSolver</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;L-BFGS-B solver.</span>

<span class="sd">  L-BFGS-B is a version of L-BFGS that incorporates box constraints on</span>
<span class="sd">  variables.</span>

<span class="sd">  Attributes:</span>
<span class="sd">    fun: a smooth function of the form ``fun(x, *args, **kwargs)``.</span>
<span class="sd">    value_and_grad: whether ``fun`` just returns the value (False) or both the</span>
<span class="sd">      value and gradient (True). See base.make_funs_with_aux for details.</span>
<span class="sd">    has_aux: whether ``fun`` outputs auxiliary data or not. If ``has_aux`` is</span>
<span class="sd">      False, ``fun`` is expected to be  scalar-valued. If ``has_aux`` is True,</span>
<span class="sd">      then we have one of the following two cases. If ``value_and_grad`` is</span>
<span class="sd">      False, the output should be ``value, aux = fun(...)``. If ``value_and_grad</span>
<span class="sd">      == True``, the output should be ``(value, aux), grad = fun(...)``. At each</span>
<span class="sd">      iteration of the algorithm, the auxiliary outputs are stored in</span>
<span class="sd">      ``state.aux``.</span>
<span class="sd">    maxiter: maximum number of proximal gradient descent iterations.</span>
<span class="sd">    tol: tolerance of the stopping criterion.</span>
<span class="sd">    stepsize: a stepsize to use (if &lt;= 0, use backtracking line search), or a</span>
<span class="sd">      callable specifying the **positive** stepsize to use at each iteration.</span>
<span class="sd">    linesearch_init: strategy for line-search initialization. By default, it</span>
<span class="sd">      will use &quot;increase&quot;, which will increased the step-size by a factor of</span>
<span class="sd">      `increase_factor` at each iteration if the step-size is larger than</span>
<span class="sd">      `min_stepsize`, and set it to `max_stepsize` otherwise. Other choices are</span>
<span class="sd">      &quot;max&quot;, that initializes the step-size to `max_stepsize` at every</span>
<span class="sd">      iteration, and &quot;current&quot;, that uses the step-size from the previous</span>
<span class="sd">      iteration.</span>
<span class="sd">    stop_if_linesearch_fails: whether to stop iterations if the line search</span>
<span class="sd">      fails. When True, this matches the behavior of core JAX.</span>
<span class="sd">    condition: Deprecated. Condition used to select the stepsize when using</span>
<span class="sd">      backtracking linesearch</span>
<span class="sd">    maxls: maximum number of iterations to use in the line search.</span>
<span class="sd">    decrease_factor: Deprecated. factor by which to decrease the stepsize during</span>
<span class="sd">      backtracking line search (default: 0.8).</span>
<span class="sd">    increase_factor: factor by which to increase the stepsize during line search</span>
<span class="sd">      (default: 1.5).</span>
<span class="sd">    max_stepsize: upper bound on stepsize.</span>
<span class="sd">    min_stepsize: lower bound on stepsize.</span>
<span class="sd">    history_size: size of the memory to use.</span>
<span class="sd">    use_gamma: whether to initialize the Hessian approximation with gamma *</span>
<span class="sd">      theta, where gamma is chosen following equation (7.20) of &#39;Numerical</span>
<span class="sd">      Optimization&#39; [2]. If use_gamma is set to False, theta is used as</span>
<span class="sd">      initialization.</span>
<span class="sd">    implicit_diff: whether to enable implicit diff or autodiff of unrolled</span>
<span class="sd">      iterations.</span>
<span class="sd">    implicit_diff_solve: the linear system solver to use.</span>
<span class="sd">    jit: whether to JIT-compile the optimization loop (default: True).</span>
<span class="sd">    unroll: whether to unroll the optimization loop (default: &quot;auto&quot;).</span>
<span class="sd">    verbose: whether to print error on every iteration or not.</span>
<span class="sd">      Warning: verbose=True will automatically disable jit.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="n">fun</span><span class="p">:</span> <span class="n">Callable</span>  <span class="c1"># pylint: disable=g-bare-generic</span>
  <span class="n">value_and_grad</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">has_aux</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>

  <span class="n">maxiter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span>
  <span class="n">tol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-3</span>

  <span class="n">stepsize</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Any</span><span class="p">],</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="mf">0.0</span>
  <span class="n">linesearch</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;zoom&quot;</span>
  <span class="n">linesearch_init</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;increase&quot;</span>
  <span class="n">stop_if_linesearch_fails</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">condition</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># deprecated in v0.8</span>
  <span class="n">maxls</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span>
  <span class="n">decrease_factor</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># deprecated in v0.8</span>
  <span class="n">increase_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.5</span>
  <span class="n">max_stepsize</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
  <span class="c1"># FIXME: should depend on whether float32 or float64 is used.</span>
  <span class="n">min_stepsize</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span>

  <span class="n">theta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
  <span class="n">history_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span>
  <span class="n">use_gamma</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="n">implicit_diff</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="n">implicit_diff_solve</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">Any</span><span class="p">],</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="n">jit</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="n">unroll</span><span class="p">:</span> <span class="n">base</span><span class="o">.</span><span class="n">AutoOrBoolean</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span>

  <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>

  <span class="k">def</span> <span class="nf">_cond_fun</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s2">&quot; error:&quot;</span><span class="p">,</span> <span class="n">state</span><span class="o">.</span><span class="n">error</span><span class="p">)</span>
    <span class="c1"># We continue the optimization loop while the error tolerance is not met</span>
    <span class="c1"># and either failed linesearch is disallowed or linesearch hasn&#39;t failed.</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">error</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">jnp</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span>
        <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">stop_if_linesearch_fails</span><span class="p">,</span> <span class="o">~</span><span class="n">state</span><span class="o">.</span><span class="n">failed_linesearch</span><span class="p">)</span>

<div class="viewcode-block" id="LBFGSB.init_state"><a class="viewcode-back" href="../../../_autosummary/jaxopt.LBFGSB.html#jaxopt.LBFGSB.init_state">[docs]</a>  <span class="k">def</span> <span class="nf">init_state</span><span class="p">(</span>
      <span class="bp">self</span><span class="p">,</span>
      <span class="n">init_params</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
      <span class="n">bounds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
      <span class="o">*</span><span class="n">args</span><span class="p">,</span>
      <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LbfgsbState</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the solver state.</span>

<span class="sd">    Args:</span>
<span class="sd">      init_params: pytree containing the initial parameters.</span>
<span class="sd">      bounds: an optional tuple `(lb, ub)` of pytrees with structure identical</span>
<span class="sd">        to `init_params`, representing box constraints.</span>
<span class="sd">      *args: additional positional arguments to be passed to ``fun``.</span>
<span class="sd">      **kwargs: additional keyword arguments to be passed to ``fun``.</span>

<span class="sd">    Returns:</span>
<span class="sd">      state</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">init_params</span><span class="p">,</span> <span class="n">base</span><span class="o">.</span><span class="n">OptStep</span><span class="p">):</span>
      <span class="c1"># `init_params` can either be a pytree or an OptStep object</span>
      <span class="n">state_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
          <span class="n">s_history</span><span class="o">=</span><span class="n">init_params</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">s_history</span><span class="p">,</span>
          <span class="n">y_history</span><span class="o">=</span><span class="n">init_params</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">y_history</span><span class="p">,</span>
          <span class="n">iter_num</span><span class="o">=</span><span class="n">init_params</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">iter_num</span><span class="p">,</span>
          <span class="n">stepsize</span><span class="o">=</span><span class="n">init_params</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">stepsize</span><span class="p">,</span>
          <span class="n">num_updates</span><span class="o">=</span><span class="n">init_params</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">num_updates</span><span class="p">,</span>
          <span class="n">theta</span><span class="o">=</span><span class="n">init_params</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span>
      <span class="p">)</span>
      <span class="n">init_params</span> <span class="o">=</span> <span class="n">init_params</span><span class="o">.</span><span class="n">params</span>
      <span class="n">dtype</span> <span class="o">=</span> <span class="n">tree_single_dtype</span><span class="p">(</span><span class="n">init_params</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">dtype</span> <span class="o">=</span> <span class="n">tree_single_dtype</span><span class="p">(</span><span class="n">init_params</span><span class="p">)</span>
      <span class="n">state_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
          <span class="n">s_history</span><span class="o">=</span><span class="n">init_history</span><span class="p">(</span><span class="n">init_params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">history_size</span><span class="p">),</span>
          <span class="n">y_history</span><span class="o">=</span><span class="n">init_history</span><span class="p">(</span><span class="n">init_params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">history_size</span><span class="p">),</span>
          <span class="n">iter_num</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
          <span class="n">stepsize</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_stepsize</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">),</span>
          <span class="n">num_updates</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
          <span class="n">theta</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">),</span>
      <span class="p">)</span>

    <span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">aux</span><span class="p">),</span> <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_value_and_grad_with_aux</span><span class="p">(</span>
        <span class="n">init_params</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">bounds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">inf</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">init_params</span><span class="p">),</span>
                <span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">inf</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">init_params</span><span class="p">))</span>
    <span class="n">init_error</span> <span class="o">=</span> <span class="n">_get_error</span><span class="p">(</span><span class="n">init_params</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="o">*</span><span class="n">bounds</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">LbfgsbState</span><span class="p">(</span>
        <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span>
        <span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">,</span>
        <span class="n">error</span><span class="o">=</span><span class="n">init_error</span><span class="p">,</span>
        <span class="o">**</span><span class="n">state_kwargs</span><span class="p">,</span>
        <span class="n">aux</span><span class="o">=</span><span class="n">aux</span><span class="p">,</span>
        <span class="n">failed_linesearch</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="kc">False</span><span class="p">),</span>
        <span class="n">num_fun_eval</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">base</span><span class="o">.</span><span class="n">NUM_EVAL_DTYPE</span><span class="p">),</span>
        <span class="n">num_grad_eval</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">base</span><span class="o">.</span><span class="n">NUM_EVAL_DTYPE</span><span class="p">),</span>
        <span class="n">num_linesearch_iter</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">base</span><span class="o">.</span><span class="n">NUM_EVAL_DTYPE</span><span class="p">)</span>
    <span class="p">)</span></div>

<div class="viewcode-block" id="LBFGSB.update"><a class="viewcode-back" href="../../../_autosummary/jaxopt.LBFGSB.html#jaxopt.LBFGSB.update">[docs]</a>  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span>
      <span class="bp">self</span><span class="p">,</span>
      <span class="n">params</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
      <span class="n">state</span><span class="p">:</span> <span class="n">LbfgsbState</span><span class="p">,</span>
      <span class="n">bounds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
      <span class="o">*</span><span class="n">args</span><span class="p">,</span>
      <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">base</span><span class="o">.</span><span class="n">OptStep</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Performs one iteration of LBFGS.</span>

<span class="sd">    Args:</span>
<span class="sd">      params: pytree containing the parameters.</span>
<span class="sd">      state: named tuple containing the solver state.</span>
<span class="sd">      bounds: an optional tuple `(lb, ub)` of pytrees with structure identical</span>
<span class="sd">        to `init_params`, representing box constraints.</span>
<span class="sd">      *args: additional positional arguments to be passed to ``fun``.</span>
<span class="sd">      **kwargs: additional keyword arguments to be passed to ``fun``.</span>

<span class="sd">    Returns:</span>
<span class="sd">      (params, state)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">base</span><span class="o">.</span><span class="n">OptStep</span><span class="p">):</span>
      <span class="n">params</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">params</span>
    <span class="k">if</span> <span class="n">bounds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">bounds</span> <span class="o">=</span> <span class="p">(</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">inf</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">params</span><span class="p">),</span>
                <span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">inf</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">params</span><span class="p">))</span>
    <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span> <span class="o">=</span> <span class="n">bounds</span>
    <span class="n">x_flat</span> <span class="o">=</span> <span class="n">_flatten_and_concat</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">g_flat</span> <span class="o">=</span> <span class="n">_flatten_and_concat</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
    <span class="n">lower_flat</span> <span class="o">=</span> <span class="n">_flatten_and_concat</span><span class="p">(</span><span class="n">lower</span><span class="p">)</span>
    <span class="n">upper_flat</span> <span class="o">=</span> <span class="n">_flatten_and_concat</span><span class="p">(</span><span class="n">upper</span><span class="p">)</span>
    <span class="n">s_history_flat</span> <span class="o">=</span> <span class="n">_flatten_and_concat</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">s_history</span><span class="p">,</span> <span class="n">batch_ndims</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y_history_flat</span> <span class="o">=</span> <span class="n">_flatten_and_concat</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">y_history</span><span class="p">,</span> <span class="n">batch_ndims</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Construct the limited-memory BFGS matrix from [1], section 3.</span>
    <span class="n">w_flat</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
        <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">y_history_flat</span><span class="p">,</span> <span class="n">state</span><span class="o">.</span><span class="n">theta</span> <span class="o">*</span> <span class="n">s_history_flat</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
    <span class="n">s_dot_yt</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">s_history_flat</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">y_history_flat</span><span class="p">))</span>

    <span class="c1"># Pad extra history dimensions with constants on the diagonal to ensure</span>
    <span class="c1"># invertability while maintaining constant array sizes for JIT compilation.</span>
    <span class="c1"># The corresponding dimensions of the inverted matrix are multiplied by zero</span>
    <span class="c1"># downstream. If possible, the constants are chosen to equal an existing</span>
    <span class="c1"># nonzero element of the diagonal, for numerical stability.</span>
    <span class="n">history_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">history_size</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">state</span><span class="o">.</span><span class="n">num_updates</span>
    <span class="n">diagonal_ones</span> <span class="o">=</span> <span class="n">history_mask</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">x_flat</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">diag_indices</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">history_size</span><span class="p">)</span>
    <span class="n">prev_ind</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">num_updates</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">history_size</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">zeros</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">s_dot_yt</span><span class="p">)</span>
    <span class="n">fill_diag_syt</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">num_updates</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">s_dot_yt</span><span class="p">[</span><span class="n">prev_ind</span><span class="p">,</span>
                                                              <span class="n">prev_ind</span><span class="p">],</span> <span class="mf">1.</span><span class="p">)</span>
    <span class="n">diag_adj</span> <span class="o">=</span> <span class="n">zeros</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">diagonal_ones</span> <span class="o">*</span> <span class="n">fill_diag_syt</span><span class="p">)</span>

    <span class="c1"># (m, m) tril with zeros on the diagonal</span>
    <span class="n">lower_tril</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">s_dot_yt</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># (m, m) diagonal</span>
    <span class="n">diag</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">s_dot_yt</span><span class="p">))</span> <span class="o">+</span> <span class="n">diag_adj</span>

    <span class="n">s_dot_st</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">s_history_flat</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">s_history_flat</span><span class="p">))</span>
    <span class="n">fill_diag_sst</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">num_updates</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">s_dot_st</span><span class="p">[</span><span class="n">prev_ind</span><span class="p">,</span>
                                                              <span class="n">prev_ind</span><span class="p">],</span> <span class="mf">1.</span><span class="p">)</span>
    <span class="n">sst_adj</span> <span class="o">=</span> <span class="n">zeros</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">diagonal_ones</span> <span class="o">*</span> <span class="n">fill_diag_sst</span><span class="p">)</span> <span class="o">*</span> <span class="n">state</span><span class="o">.</span><span class="n">theta</span>
    <span class="n">m_inv</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>  <span class="c1"># Equation 3.4 of [1].</span>
        <span class="p">[</span>
            <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">diag</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">lower_tril</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">lower_tril</span><span class="p">,</span> <span class="n">sst_adj</span> <span class="o">+</span> <span class="n">state</span><span class="o">.</span><span class="n">theta</span> <span class="o">*</span> <span class="n">s_dot_st</span><span class="p">],</span>
                            <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">],</span>
        <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">m_inv</span><span class="p">)</span>

    <span class="n">x_cauchy</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">active_ind</span> <span class="o">=</span> <span class="n">_find_cauchy_point</span><span class="p">(</span>
        <span class="n">x_flat</span><span class="p">,</span> <span class="n">g_flat</span><span class="p">,</span> <span class="n">lower_flat</span><span class="p">,</span> <span class="n">upper_flat</span><span class="p">,</span> <span class="n">state</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span> <span class="n">w_flat</span><span class="p">,</span> <span class="n">m</span>
    <span class="p">)</span>
    <span class="n">x_subspace_min</span> <span class="o">=</span> <span class="n">_minimize_subspace</span><span class="p">(</span>
        <span class="n">x_flat</span><span class="p">,</span>
        <span class="n">g_flat</span><span class="p">,</span>
        <span class="n">lower_flat</span><span class="p">,</span>
        <span class="n">upper_flat</span><span class="p">,</span>
        <span class="n">x_cauchy</span><span class="p">,</span>
        <span class="n">c</span><span class="p">,</span>
        <span class="n">state</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span>
        <span class="n">w_flat</span><span class="p">,</span>
        <span class="n">m</span><span class="p">,</span>
        <span class="n">active_ind</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">descent_direction</span> <span class="o">=</span> <span class="n">_split_and_pack_like</span><span class="p">(</span><span class="n">x_subspace_min</span> <span class="o">-</span> <span class="n">x_flat</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

    <span class="n">use_linesearch</span> <span class="o">=</span> <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stepsize</span><span class="p">,</span> <span class="n">Callable</span><span class="p">)</span> <span class="ow">and</span>
                      <span class="bp">self</span><span class="o">.</span><span class="n">stepsize</span> <span class="o">&lt;=</span> <span class="mf">0.</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">use_linesearch</span><span class="p">:</span>
      <span class="n">init_stepsize</span> <span class="o">=</span> <span class="n">_init_stepsize</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">linesearch_init</span><span class="p">,</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">max_stepsize</span><span class="p">,</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">min_stepsize</span><span class="p">,</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">increase_factor</span><span class="p">,</span>
          <span class="n">state</span><span class="o">.</span><span class="n">stepsize</span><span class="p">,</span>
      <span class="p">)</span>
      <span class="n">new_stepsize</span><span class="p">,</span> <span class="n">ls_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_ls</span><span class="p">(</span>
          <span class="n">init_stepsize</span><span class="p">,</span>
          <span class="n">params</span><span class="p">,</span>
          <span class="n">value</span><span class="o">=</span><span class="n">state</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
          <span class="n">grad</span><span class="o">=</span><span class="n">state</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span>
          <span class="n">descent_direction</span><span class="o">=</span><span class="n">descent_direction</span><span class="p">,</span>
          <span class="n">fun_args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
          <span class="n">fun_kwargs</span><span class="o">=</span><span class="n">kwargs</span><span class="p">,</span>
      <span class="p">)</span>
      <span class="n">new_params</span> <span class="o">=</span> <span class="n">ls_state</span><span class="o">.</span><span class="n">params</span>
      <span class="n">new_value</span> <span class="o">=</span> <span class="n">ls_state</span><span class="o">.</span><span class="n">value</span>
      <span class="n">new_grad</span> <span class="o">=</span> <span class="n">ls_state</span><span class="o">.</span><span class="n">grad</span>
      <span class="n">new_aux</span> <span class="o">=</span> <span class="n">ls_state</span><span class="o">.</span><span class="n">aux</span>
      <span class="n">failed_linesearch</span> <span class="o">=</span> <span class="n">ls_state</span><span class="o">.</span><span class="n">failed</span>
      <span class="n">new_num_linesearch_iter</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">num_linesearch_iter</span> <span class="o">+</span> <span class="n">ls_state</span><span class="o">.</span><span class="n">iter_num</span>
      <span class="n">new_num_grad_eval</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">num_grad_eval</span> <span class="o">+</span> <span class="n">ls_state</span><span class="o">.</span><span class="n">num_grad_eval</span>
      <span class="n">new_num_fun_eval</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">num_fun_eval</span> <span class="o">+</span> <span class="n">ls_state</span><span class="o">.</span><span class="n">num_fun_eval</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stepsize</span><span class="p">,</span> <span class="n">Callable</span><span class="p">):</span>
        <span class="n">new_stepsize</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stepsize</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">iter_num</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">new_stepsize</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stepsize</span>

      <span class="n">new_params</span> <span class="o">=</span> <span class="n">tree_add_scalar_mul</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">new_stepsize</span><span class="p">,</span> <span class="n">descent_direction</span><span class="p">)</span>
      <span class="n">new_params</span> <span class="o">=</span> <span class="n">tree_map</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">,</span> <span class="n">new_params</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">)</span>

      <span class="p">(</span><span class="n">new_value</span><span class="p">,</span> <span class="n">new_aux</span><span class="p">),</span> <span class="n">new_grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_value_and_grad_with_aux</span><span class="p">(</span>
          <span class="n">new_params</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
      <span class="p">)</span>
      <span class="n">new_num_grad_eval</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">num_grad_eval</span> <span class="o">+</span> <span class="mi">1</span>
      <span class="n">new_num_fun_eval</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">num_fun_eval</span> <span class="o">+</span> <span class="mi">1</span>
      <span class="n">new_num_linesearch_iter</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">num_linesearch_iter</span>
      <span class="n">failed_linesearch</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">s</span> <span class="o">=</span> <span class="n">tree_sub</span><span class="p">(</span><span class="n">new_params</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">tree_sub</span><span class="p">(</span><span class="n">new_grad</span><span class="p">,</span> <span class="n">state</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
    <span class="n">curvature</span> <span class="o">=</span> <span class="n">tree_vdot</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_gamma</span><span class="p">:</span>
      <span class="n">gamma_inv</span> <span class="o">=</span> <span class="n">tree_vdot</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">curvature</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">gamma_inv</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">curvature</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="n">history_ind</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">num_updates</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">history_size</span>
    <span class="p">(</span><span class="n">new_s_history</span><span class="p">,</span> <span class="n">new_y_history</span><span class="p">,</span> <span class="n">new_theta</span><span class="p">,</span> <span class="n">new_num_updates</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span>
            <span class="n">curvature</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">,</span>
            <span class="k">lambda</span> <span class="n">sh</span><span class="p">,</span> <span class="n">yh</span><span class="p">:</span> <span class="p">(</span>  <span class="c1"># pylint: disable=g-long-lambda</span>
                <span class="n">update_history</span><span class="p">(</span><span class="n">sh</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">history_ind</span><span class="p">),</span>
                <span class="n">update_history</span><span class="p">(</span><span class="n">yh</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">history_ind</span><span class="p">),</span>
                <span class="n">gamma_inv</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span>
                <span class="n">state</span><span class="o">.</span><span class="n">num_updates</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="k">lambda</span> <span class="n">sh</span><span class="p">,</span> <span class="n">yh</span><span class="p">:</span> <span class="p">(</span>  <span class="c1"># pylint: disable=g-long-lambda</span>
                <span class="n">sh</span><span class="p">,</span>
                <span class="n">yh</span><span class="p">,</span>
                <span class="n">state</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span>
                <span class="n">state</span><span class="o">.</span><span class="n">num_updates</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">state</span><span class="o">.</span><span class="n">s_history</span><span class="p">,</span>
            <span class="n">state</span><span class="o">.</span><span class="n">y_history</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="p">)</span>

    <span class="n">error</span> <span class="o">=</span> <span class="n">_get_error</span><span class="p">(</span><span class="n">new_params</span><span class="p">,</span> <span class="n">new_grad</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">)</span>
    <span class="n">new_state</span> <span class="o">=</span> <span class="n">LbfgsbState</span><span class="p">(</span>
        <span class="n">iter_num</span><span class="o">=</span><span class="n">state</span><span class="o">.</span><span class="n">iter_num</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">value</span><span class="o">=</span><span class="n">new_value</span><span class="p">,</span>
        <span class="n">grad</span><span class="o">=</span><span class="n">new_grad</span><span class="p">,</span>
        <span class="n">stepsize</span><span class="o">=</span><span class="n">new_stepsize</span><span class="p">,</span>
        <span class="n">error</span><span class="o">=</span><span class="n">error</span><span class="p">,</span>
        <span class="n">s_history</span><span class="o">=</span><span class="n">new_s_history</span><span class="p">,</span>
        <span class="n">y_history</span><span class="o">=</span><span class="n">new_y_history</span><span class="p">,</span>
        <span class="n">num_updates</span><span class="o">=</span><span class="n">new_num_updates</span><span class="p">,</span>
        <span class="n">theta</span><span class="o">=</span><span class="n">new_theta</span><span class="p">,</span>
        <span class="n">aux</span><span class="o">=</span><span class="n">new_aux</span><span class="p">,</span>
        <span class="n">failed_linesearch</span><span class="o">=</span><span class="n">failed_linesearch</span><span class="p">,</span>
        <span class="n">num_grad_eval</span><span class="o">=</span><span class="n">new_num_grad_eval</span><span class="p">,</span>
        <span class="n">num_fun_eval</span><span class="o">=</span><span class="n">new_num_fun_eval</span><span class="p">,</span>
        <span class="n">num_linesearch_iter</span><span class="o">=</span><span class="n">new_num_linesearch_iter</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">base</span><span class="o">.</span><span class="n">OptStep</span><span class="p">(</span><span class="n">new_params</span><span class="p">,</span> <span class="n">new_state</span><span class="p">)</span></div>

  <span class="k">def</span> <span class="nf">_fixed_point_fun</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sol</span><span class="p">,</span> <span class="n">bounds</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
    <span class="n">step</span> <span class="o">=</span> <span class="n">tree_sub</span><span class="p">(</span><span class="n">sol</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grad_fun</span><span class="p">(</span><span class="n">sol</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">projection</span><span class="o">.</span><span class="n">projection_box</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">bounds</span><span class="p">)</span>

<div class="viewcode-block" id="LBFGSB.optimality_fun"><a class="viewcode-back" href="../../../_autosummary/jaxopt.LBFGSB.html#jaxopt.LBFGSB.optimality_fun">[docs]</a>  <span class="k">def</span> <span class="nf">optimality_fun</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sol</span><span class="p">,</span> <span class="n">bounds</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Optimality function mapping compatible with `@custom_root`.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">bounds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_value_and_grad_fun</span><span class="p">(</span><span class="n">sol</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">fp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fixed_point_fun</span><span class="p">(</span><span class="n">sol</span><span class="p">,</span> <span class="n">bounds</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tree_sub</span><span class="p">(</span><span class="n">fp</span><span class="p">,</span> <span class="n">sol</span><span class="p">)</span></div>

  <span class="k">def</span> <span class="nf">_value_and_grad_fun</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">base</span><span class="o">.</span><span class="n">OptStep</span><span class="p">):</span>
      <span class="n">params</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">params</span>
    <span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">_</span><span class="p">),</span> <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_value_and_grad_with_aux</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">value</span><span class="p">,</span> <span class="n">grad</span>

  <span class="k">def</span> <span class="nf">_grad_fun</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_value_and_grad_fun</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__post_init__</span><span class="p">()</span>

    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_value_and_grad_with_aux</span> <span class="o">=</span> <span class="n">base</span><span class="o">.</span><span class="n">_make_funs_with_aux</span><span class="p">(</span>
        <span class="n">fun</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fun</span><span class="p">,</span>
        <span class="n">value_and_grad</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">,</span>
        <span class="n">has_aux</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">has_aux</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Sets up reference signature.</span>
    <span class="n">fun</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fun</span><span class="p">,</span> <span class="s2">&quot;subfun&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">fun</span><span class="p">)</span>
    <span class="n">signature</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">fun</span><span class="p">)</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">signature</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
    <span class="n">new_param</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;bounds&quot;</span><span class="p">,</span>
                                  <span class="n">kind</span><span class="o">=</span><span class="n">inspect</span><span class="o">.</span><span class="n">Parameter</span><span class="o">.</span><span class="n">POSITIONAL_OR_KEYWORD</span><span class="p">)</span>
    <span class="n">parameters</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">new_param</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">reference_signature</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">Signature</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>


    <span class="n">unroll</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_unroll_option</span><span class="p">()</span>
    <span class="n">linesearch_solver</span> <span class="o">=</span> <span class="n">_setup_linesearch</span><span class="p">(</span>
        <span class="n">linesearch</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">linesearch</span><span class="p">,</span>
        <span class="n">fun</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_value_and_grad_with_aux</span><span class="p">,</span>
        <span class="n">value_and_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">maxlsiter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">maxls</span><span class="p">,</span>
        <span class="n">max_stepsize</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_stepsize</span><span class="p">,</span>
        <span class="n">jit</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span>
        <span class="n">unroll</span><span class="o">=</span><span class="n">unroll</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">run_ls</span> <span class="o">=</span> <span class="n">linesearch_solver</span><span class="o">.</span><span class="n">run</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">condition</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Argument condition is deprecated&quot;</span><span class="p">,</span> <span class="ne">DeprecationWarning</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">decrease_factor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
          <span class="s2">&quot;Argument decrease_factor is deprecated&quot;</span><span class="p">,</span> <span class="ne">DeprecationWarning</span>
      <span class="p">)</span></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021-2022, the JAXopt authors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>