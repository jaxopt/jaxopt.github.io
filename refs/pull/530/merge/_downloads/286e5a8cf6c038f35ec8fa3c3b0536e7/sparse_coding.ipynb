{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Sparse coding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from absl import app\nfrom absl import flags\n\nimport functools\nfrom typing import Any\nfrom typing import Callable\nfrom typing import Mapping\nfrom typing import Optional\n\nimport unittest\n\nimport jax\nimport jax.numpy as jnp\nfrom jax.nn import softplus\n\nfrom jaxopt import loss\nfrom jaxopt import OptaxSolver\nfrom jaxopt import projection\nfrom jaxopt import prox\nfrom jaxopt import ProximalGradient\n\nimport optax\nfrom sklearn import datasets\n\n\nflags.DEFINE_integer(\"num_examples\", 74, \"NUmber of examples.\")\nflags.DEFINE_integer(\"num_components\", 7, \"Number of atoms in dictionnary.\")\nflags.DEFINE_integer(\"num_features\", 13, \"Number of features.\")\nflags.DEFINE_integer(\"sparse_coding_maxiter\", 100, \"Number of iterations for sparse coding.\")\nflags.DEFINE_integer(\"maxiter\", 10, \"Number of iterations of the outer loop.\")\nflags.DEFINE_float(\"elastic_penalty\", 0.01, \"Strength of L2 penalty relative to L1.\")\nflags.DEFINE_float(\"regularization\", 0.01, \"Regularization strength of elastic penalty.\")\nflags.DEFINE_enum(\"reconstruction_loss\", \"squared\", [\"squared\", \"abs\", \"huber\"], \"Loss used to build dictionnary.\")\nFLAGS = flags.FLAGS\n\n\ndef dictionary_loss(\n    codes: jnp.ndarray,\n    dictionary: jnp.ndarray,\n    data: jnp.ndarray,\n    reconstruction_loss_fun: Callable[[jnp.ndarray, jnp.ndarray],\n                                      jnp.ndarray] = None):\n  \"\"\"Computes reconstruction loss between data and dict/codes using loss fun.\n\n  Args:\n    codes: a n_samples x components array of codes.\n    dictionary: a components x dimension array\n    data: a n_samples x dimension array\n    reconstruction_loss_fun: a callable loss(x, y) -> a real number, where\n      x and y are either entries, slices or the matrices themselves.\n      Set to 1/2 squared L2 norm of difference by default.\n\n  Returns:\n    a float, the reconstruction loss.\n  \"\"\"\n  if reconstruction_loss_fun is None:\n    reconstruction_loss_fun = lambda x, y: 0.5 * jnp.sum((x - y)**2)\n  pred = codes @ dictionary\n  return reconstruction_loss_fun(data, pred)\n\n\ndef make_task_driven_dictionary_learner(\n    task_loss_fun: Optional[Callable[[Any, Any, Any, Any], float]] = None,\n    reconstruction_loss_fun: Optional[Callable[[jnp.ndarray, jnp.ndarray],\n                                               jnp.ndarray]] = None,\n    optimizer = None,\n    sparse_coding_kw: Mapping[str, Any] = None,\n    **kwargs):\n  \"\"\"Makes a task-driven sparse dictionary learning solver.\n\n  Returns a jaxopt solver, using either an optax optimizer or jaxopt prox\n  gradient optimizer, to compute, given data, a dictionary whose corresponding\n  codes minimizes a given task loss. The solver is defined through the task loss\n  function, a reconstruction loss function, and an optimizer. Additional\n  parameters can be passed on to lower level functions, notably the computation\n  of sparse codes and optimizer parameters.\n\n  Args:\n    task_loss_fun: loss as specified on (codes, dict, task_vars, params) that\n      supplements the usual reconstruction loss formulation. If None, only\n      dictionary learning is carried out, i.e. that term is assumed to be 0.\n    reconstruction_loss_fun: entry (or slice-) wise loss function, set to be\n      the Frobenius norm between matrices, || . - . ||^2 by default.\n    optimizer: optax optimizer. fall back on jaxopt proxgrad if None.\n    sparse_coding_kw: Jaxopt arguments to be passed to the proximal descent\n      algorithm computing codes, sparse_coding.\n    **kwargs: passed onto _task_sparse_dictionary_learning function.\n\n  Returns:\n    Function to learn dictionary from data, number of components and\n      elastic net regularization, using initialization for dictionary,\n      parameters for task and task variables initialization.\n  \"\"\"\n  def learner(data: jnp.ndarray,\n              n_components: int,\n              regularization: float,\n              elastic_penalty: float,\n              task_vars_init: jnp.ndarray = None,\n              task_params: jnp.ndarray = None,\n              dic_init: Optional[jnp.ndarray] = None):\n\n    return _task_sparse_dictionary_learning(data, n_components, regularization,\n                                            elastic_penalty, task_vars_init,\n                                            optimizer,\n                                            dic_init, task_params,\n                                            reconstruction_loss_fun,\n                                            task_loss_fun,\n                                            sparse_coding_kw, **kwargs)\n\n  return learner\n\n\ndef _task_sparse_dictionary_learning(\n    data: jnp.ndarray,\n    n_components: int,\n    regularization: float,\n    elastic_penalty: float,\n    task_vars_init: jnp.ndarray,\n    optimizer=None,\n    dic_init: Optional[jnp.ndarray] = None,\n    task_params: jnp.ndarray = None,\n    reconstruction_loss_fun: Callable[[jnp.ndarray, jnp.ndarray],\n                                      jnp.ndarray] = None,\n    task_loss_fun: Callable[[Any, Any, Any, Any], float] = None,\n    sparse_coding_kw: Mapping[str, Any] = None,\n    maxiter: int = 100):\n  r\"\"\"Computes task driven dictionary, w. implicitly defined sparse codes.\n\n  Given a N x d ``data`` matrix, solves a bilevel optimization problem by\n  seeking a dictionary ``dic`` of size ``n_components`` x ``d`` such that,\n  defining implicitly\n  ``codes = sparse_coding(dic, (data, regularization, elastic_penalty))``\n  one has that ``dic`` minimizes\n  ``task_loss(codes, dic, task_var, task_params)``,\n  if such as ``task_loss`` was passed on. If ``task_loss`` is ``None``, then\n  ``task_loss`` is replaced by default by\n  ``dictionary_loss(codes, (dic, data))``.\n\n  Args:\n    data: N x d jnp.ndarray, data matrix with N samples of d features.\n    n_components: int, number of atoms in dictionary.\n    regularization: regularization strength of elastic penalty.\n    elastic_penalty: strength of L2 penalty relative to L1.\n    task_vars_init: initializer for task related optimization variables.\n    optimizer: If None, falls back on jaxopt proximal gradient (with sphere\n      projection for ``dic``). If not ``None``, use that algorithm's method with\n      a normalized dictionary.\n    dic_init: initialization for dictionary; that returned by SVD by default.\n    reconstruction_loss_fun: loss to be applied to compute reconstruction error.\n    task_params: auxiliary parameters to define task loss, typically data.\n    task_loss_fun: task driven loss for codes and dictionary using task_vars and\n      task_params.\n    sparse_coding_kw: parameters passed on to jaxopt prox gradient solver to\n      compute codes.\n    maxiter: maximal number of iterations of the outer loop.\n\n  Returns:\n    A``n_components x d`` matrix, the ``dic`` solution found by the algorithm,\n    as well as task variables if task was provided.\n  \"\"\"\n\n  if dic_init is None:\n    _, _, dic_init = jax.scipy.linalg.svd(data, False)\n    dic_init = dic_init[:n_components, :]\n\n  has_task = task_loss_fun is not None\n\n  # Loss function, dictionary learning in addition to task driven loss\n  def loss_fun(params, hyper_params):\n    dic, task_vars = params\n    coding_params, task_params = hyper_params\n    codes = sparse_coding(\n        dic,\n        coding_params,\n        reconstruction_loss_fun=reconstruction_loss_fun,\n        sparse_coding_kw=sparse_coding_kw)\n    if optimizer is not None:\n      dic = projection.projection_l2_sphere(dic)\n\n    if has_task:\n      loss = task_loss_fun(codes, dic, task_vars, task_params)\n    else:\n      loss = dictionary_loss(codes, dic, data, reconstruction_loss_fun)\n    return loss, codes\n\n  def prox_dic(params, hyper, step):\n    # Here projection/prox is only applied on the dictionary.\n    del hyper, step\n    dic, task_vars = params\n    return projection.projection_l2_sphere(dic), task_vars\n\n  if optimizer is None:\n    solver = ProximalGradient(fun=loss_fun, prox=prox_dic, has_aux=True)\n    params = (dic_init, task_vars_init)\n    state = solver.init_state(\n      params,\n      None,\n      hyper_params=((data, regularization, elastic_penalty), task_params),\n    )\n\n    for _ in range(maxiter):\n      params, state = solver.update(\n          params, state, None,\n          ((data, regularization, elastic_penalty), task_params))\n\n      # Normalize dictionary before returning it.\n      dic, task_vars = prox_dic(params, None, None)\n\n  else:\n    solver = OptaxSolver(opt=optimizer, fun=loss_fun, has_aux=True)\n    params = (dic_init, task_vars_init)\n    state = solver.init_state(\n      params,\n      hyper_params=((data, regularization, elastic_penalty), task_params),\n    )\n\n    for _ in range(maxiter):\n      params, state = solver.update(\n          params, state,\n          ((data, regularization, elastic_penalty), task_params))\n\n      # Normalize dictionary before returning it.\n      dic, task_vars = prox_dic(params, None, None)\n\n  if has_task:\n    return dic, task_vars\n  return dic\n\n\ndef sparse_coding(dic, params, reconstruction_loss_fun=None,\n                  sparse_coding_kw=None, codes_init=None):\n  \"\"\"Computes optimal codes for data given a dictionary dic using params.\"\"\"\n  sparse_coding_kw = {} if sparse_coding_kw is None else sparse_coding_kw\n  loss_fun = functools.partial(dictionary_loss,\n                               reconstruction_loss_fun=reconstruction_loss_fun)\n  data, regularization, elastic_penalty = params\n  n_components, _ = dic.shape\n  n_points, _ = data.shape\n\n  if codes_init is None:\n    codes_init = jnp.zeros((n_points, n_components))\n\n  solver = ProximalGradient(\n      fun=loss_fun,\n      prox=prox.prox_elastic_net,\n      **sparse_coding_kw)\n\n  codes = solver.run(codes_init, [regularization, elastic_penalty],\n                     dic, data).params\n  return codes\n\n\ndef main(argv):\n  del argv\n\n  # needed for asserts\n  tc = unittest.TestCase()\n\n  N = FLAGS.num_examples\n  k = FLAGS.num_components\n  d = FLAGS.num_features\n  # X is N x d\n  # dic is k x d\n  X, dictionary_0, codes_0 = datasets.make_sparse_coded_signal(\n      n_samples=N,\n      n_components=k,\n      n_features=d,\n      n_nonzero_coefs=k//2,\n      random_state=0,\n  )\n  X = X.T # bug in https://github.com/scikit-learn/scikit-learn/issues/19894\n  X = .1 * X + .0001 * jax.random.normal(jax.random.PRNGKey(0), (N, d))\n\n  if FLAGS.reconstruction_loss == \"squared\":\n    reconstruction_loss_fun = None\n  elif FLAGS.reconstruction_loss == \"abs\":\n    reconstruction_loss_fun = lambda x, y: jnp.sum(jnp.abs(x - y)**2.1)\n  elif FLAGS.reconstruction_loss == \"huber\":\n    reconstruction_loss_fun = lambda x, y: jnp.sum(loss.huber_loss(x, y, .01))\n  else:\n    raise ValueError(f\"Unkwown reconstruction_loss {FLAGS.reconstruction_loss}\")\n\n  elastic_penalty = FLAGS.elastic_penalty\n  regularization = FLAGS.regularization\n\n  # slightly complicated Vanilla dictionary learning when no task.\n  # complicated in the sense that Danskin is not used. Here using prox from\n  # jaxopt.\n  solver = jax.jit(\n      make_task_driven_dictionary_learner(\n          reconstruction_loss_fun=reconstruction_loss_fun, maxiter=FLAGS.maxiter,\n          sparse_coding_kw={'maxiter': FLAGS.sparse_coding_maxiter}),\n      static_argnums=(1, 8))  # n_components & reconstruction_loss_fun\n\n  print(\"Create dictionnary with no task:\", flush=True)\n  # Compute dictionary\n  dic_jop_0 = solver(\n      X,\n      n_components=k,\n      regularization=regularization,\n      elastic_penalty=elastic_penalty)\n  tc.assertEqual(dic_jop_0.shape, (k, d))\n  print(dic_jop_0)\n\n  # Test now task driven dictionary learning using *arbitrary* labels computed\n  # from initial codes. This is a binary logistic regression problem.\n  label = jnp.sum(codes_0[0:3, :], axis=0) > 0\n  def task_loss_fun(codes, dic, task_vars, task_params):\n    del dic\n    w, b = task_vars\n    logit = jnp.dot(codes, w) + b\n    return jnp.sum(\n        jnp.sum(softplus(logit) - label * logit) + 0.5 * task_params *\n        (jnp.dot(w, w) + b * b))\n\n  # Create a solver that will now use optax's Adam to learn both dic and\n  # logistic regression parameters.\n  solver = jax.jit(\n      make_task_driven_dictionary_learner(\n          task_loss_fun=task_loss_fun,\n          reconstruction_loss_fun=reconstruction_loss_fun, maxiter=FLAGS.maxiter,\n          sparse_coding_kw={'maxiter': FLAGS.sparse_coding_maxiter},\n          optimizer=optax.adam(1e-3)),\n      static_argnums=(1, 8))  # n_components & reconstruction_loss_fun\n\n  print(\"Compute task driven dictionnary:\", flush=True)\n  dic_jop_task, w_and_b = solver(\n      X,\n      n_components=k,\n      regularization=regularization,\n      elastic_penalty=elastic_penalty,\n      task_vars_init=(jnp.zeros(k), jnp.zeros(1)),\n      task_params=0.001)\n  print(dic_jop_task)\n\n  # Check we have at least improved results using the very same w_and_b\n  losses = []\n  for dic in [dic_jop_0, dic_jop_task]:\n    losses.append(\n        task_loss_fun(\n            sparse_coding(\n                dic, (X, regularization, elastic_penalty)), dic, w_and_b,\n            0.0))\n  tc.assertGreater(losses[0], losses[1])\n  print(f\"With task the loss ({losses[1]}) is smaller than without task ({losses[0]})\")\n\n\nif __name__ == \"__main__\":\n  app.run(main)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}