{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Binary kernel SVM with intercept.\n\nThe dual objective of binary kernel SVMs with an intercept contains both\nbox constraints and an equality constraint, making it challenging to solve.\nThe state-of-the-art algorithm to solve this objective is SMO (Sequential\nminimal optimization). \n\nWe nevertheless demonstrate in this example how to solve\nit by projected gradient descent, by projecting on the constraint set\nusing projection_box_section.  \n  \nSince the dual objective is a Quadratic Program we show how to solve\nit with BoxOSQP too.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from absl import app\nfrom absl import flags\n\nimport jax\nimport jax.numpy as jnp\nfrom jaxopt import projection\nfrom jaxopt import ProjectedGradient\nfrom jaxopt import BoxOSQP\n\nimport numpy as onp\nfrom sklearn import datasets\nfrom sklearn import preprocessing\nfrom sklearn import svm\n\n\nflags.DEFINE_float(\"lam\", 0.5, \"Regularization parameter. Must be positive.\")\nflags.DEFINE_float(\"tol\", 1e-6, \"Tolerance of solvers.\")\nflags.DEFINE_integer(\"num_samples\", 30, \"Size of train set.\")\nflags.DEFINE_integer(\"num_features\", 5, \"Features dimension.\")\nflags.DEFINE_bool(\"verbose\", False, \"Verbosity.\")\nFLAGS = flags.FLAGS\n\n\ndef binary_kernel_svm_skl(X, y, C):\n  print(f\"Solve SVM with sklearn.svm.SVC: \")\n  K = jnp.dot(X, X.T)\n  svc = svm.SVC(kernel=\"precomputed\", C=C, tol=FLAGS.tol).fit(K, y)\n  dual_coef = onp.zeros(K.shape[0])\n  dual_coef[svc.support_] = svc.dual_coef_[0]\n  return dual_coef\n\n\ndef binary_kernel_svm_pg(X, y, C):\n  print(f\"Solve SVM with Projected Gradient: \")\n\n  def objective_fun(beta, X, y):\n    \"\"\"Dual objective of binary kernel SVMs with intercept.\"\"\"\n    # The dual objective is:\n    # fun(beta) = 0.5 beta^T K beta - beta^T y\n    # subject to\n    # sum(beta) = 0\n    # 0 <= beta_i <= C if y_i = 1\n    # -C <= beta_i <= 0 if y_i = -1\n    # where C = 1.0 / lam\n    # and K = X X^T\n    Kbeta = jnp.dot(X, jnp.dot(X.T, beta))\n    return 0.5 * jnp.dot(beta, Kbeta) - jnp.dot(beta, y)\n\n  # Define projection operator.\n  w = jnp.ones(X.shape[0])\n\n  def proj(beta, C):\n    box_lower = jnp.where(y == 1, 0, -C)\n    box_upper = jnp.where(y == 1, C, 0)\n    proj_params = (box_lower, box_upper, w, 0.0)\n    return projection.projection_box_section(beta, proj_params)\n\n  # Run solver.\n  beta_init = jnp.ones(X.shape[0])\n  solver = ProjectedGradient(fun=objective_fun,\n                             projection=proj,\n                             tol=FLAGS.tol, maxiter=500, verbose=FLAGS.verbose)\n  beta_fit = solver.run(beta_init, hyperparams_proj=C, X=X, y=y).params\n\n  return beta_fit\n\n\ndef binary_kernel_svm_osqp(X, y, C):\n  # The dual objective is:\n  # fun(beta) = 0.5 beta^T K beta - beta^T y\n  # subject to\n  # sum(beta) = 0\n  # 0 <= beta_i <= C if y_i = 1\n  # -C <= beta_i <= 0 if y_i = -1\n  # where C = 1.0 / lam\n\n  print(f\"Solve SVM with OSQP: \")\n\n  def matvec_Q(X, beta):\n    return jnp.dot(X, jnp.dot(X.T,  beta))\n  \n  # There qre two types of constraints:\n  #   0 <= y_i * beta_i <= C     (1)\n  # and:\n  #   sum(beta) = 0              (2)\n  # The first one involves the identity matrix over the betas.\n  # The second one involves their sum (i.e dot product with vector full of 1).\n  # We take advantage of matvecs to avoid materializing A in memory.\n  # We return a tuple whose entries correspond each type of constraint.\n  def matvec_A(_, beta):\n    return beta, jnp.sum(beta)\n  \n  # l, u must have same shape than matvec_A's output.\n  l = -jax.nn.relu(-y * C), 0.\n  u =  jax.nn.relu( y * C), 0.\n\n  hyper_params = dict(params_obj=(X, -y), params_eq=None, params_ineq=(l, u))\n  osqp = BoxOSQP(matvec_Q=matvec_Q, matvec_A=matvec_A, tol=FLAGS.tol)\n  params, _ = osqp.run(init_params=None, **hyper_params)\n  beta = params.primal[0]\n\n  return beta\n\n\ndef print_svm_result(beta, threshold=1e-4):\n  # Here the vector `beta` of coefficients is signed:\n  # its sign depends of the true label of the corresponding example.\n  # Hence we use jnp.abs() to detect support vectors.\n  is_support_vectors = jnp.abs(beta) > threshold\n  print(f\"Beta: {beta}\")\n  print(f\"Support vector indices: {onp.where(is_support_vectors)[0]}\")\n  print(\"\")\n\n\ndef main(argv):\n  del argv\n\n  num_samples = FLAGS.num_samples\n  num_features = FLAGS.num_features\n\n  # Prepare data.\n  X, y = datasets.make_classification(n_samples=num_samples, n_features=num_features,\n                                      n_classes=2,\n                                      random_state=0)\n  X = preprocessing.Normalizer().fit_transform(X)\n  y = jnp.array(y * 2. - 1)  # Transform labels from {0, 1} to {-1., 1.}.\n\n  lam = FLAGS.lam\n  C = 1./ lam\n\n  # Compare the obtained dual coefficients.\n  beta_fit_osqp = binary_kernel_svm_osqp(X, y, C)\n  print_svm_result(beta_fit_osqp)\n\n  beta_fit_pg = binary_kernel_svm_pg(X, y, C)\n  print_svm_result(beta_fit_pg)\n\n  beta_fit_skl = binary_kernel_svm_skl(X, y, C)\n  print_svm_result(beta_fit_skl)\n  \n\nif __name__ == \"__main__\":\n  jax.config.update(\"jax_platform_name\", \"cpu\")\n  app.run(main)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}