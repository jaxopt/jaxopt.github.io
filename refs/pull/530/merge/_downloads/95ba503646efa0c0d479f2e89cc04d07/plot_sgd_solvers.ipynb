{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Comparison of different SGD algorithms.\n\nThe purpose of this example is to illustrate the power\nof adaptive stepsize algorithms.\n\nWe chose a classification task with classes that are easily separable\nand no regularization (interpolating regime).\n\nWe compare:\n\n* SGD with Polyak stepsize (theoretical guarantees require interpolation)\n* SGD with Armijo line search (theoretical guarantees require interpolation)\n* SGD with constant stepsize\n* RMSprop\n\nThe reported ``training loss`` is an estimation of the true training loss based\non the current minibatch.\n\nThis experiment was conducted without momentum, with popular default values for\nlearning rate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from absl import flags\n\nimport logging\nimport sys\nfrom timeit import default_timer as timer\n\nimport jax\nimport jax.numpy as jnp\n\nfrom jaxopt import loss\nfrom jaxopt import ArmijoSGD\nfrom jaxopt import PolyakSGD\nfrom jaxopt import OptaxSolver\nfrom jaxopt.tree_util import tree_l2_norm, tree_sub\n\nimport optax\nfrom flax import linen as nn\n\nimport numpy as onp\nimport matplotlib.pyplot as plt\nfrom scipy.signal import convolve\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\n\n\ndataset_names = [\n    \"mnist\", \"kmnist\", \"emnist\", \"fashion_mnist\", \"cifar10\", \"cifar100\"\n]\n\nflags.DEFINE_float(\"rmsprop_stepsize\", 1e-3, \"RMSProp stepsize\")\nflags.DEFINE_float(\"gd_stepsize\", 1e-1, \"Gradient descent stepsize\")\nflags.DEFINE_float(\"aggressiveness\", 0.9, \"Aggressiveness of line search in armijo-sgd.\")\nflags.DEFINE_float(\"max_stepsize\", 1.0, \"Maximum step size (used in polyak-sgd, armijo-sgd).\")\n\nflags.DEFINE_integer(\"maxiter\", 100, \"Maximum number of iterations.\")\nflags.DEFINE_integer(\"batch_size\", 128, \"Batch-size.\")\nflags.DEFINE_integer(\"net_width\", 4, \"Width mutiplicator of network.\")\nflags.DEFINE_enum(\"dataset\", \"fashion_mnist\", dataset_names, \"Dataset to train on.\")\n\njax.config.update(\"jax_platform_name\", \"cpu\")\nplt.rcParams.update({'font.size': 13})\n\n\n# Load dataset.\ndef load_dataset(dataset, batch_size):\n  \"\"\"Loads the dataset as a generator of batches.\"\"\"\n  train_ds, ds_info = tfds.load(f\"{dataset}:3.*.*\", split=\"train\",\n                                as_supervised=True, with_info=True)\n  train_ds = train_ds.repeat()\n  train_ds = train_ds.shuffle(10 * batch_size, seed=0)\n  train_ds = train_ds.batch(batch_size)\n  return tfds.as_numpy(train_ds), ds_info\n\n\n# Define NN architecture.\nclass CNN(nn.Module):\n  \"\"\"A simple CNN model.\"\"\"\n  num_classes: int\n  net_width: int\n\n  @nn.compact\n  def __call__(self, x):\n    x = nn.Conv(features=self.net_width, kernel_size=(3, 3))(x)\n    x = nn.relu(x)\n    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n    x = nn.Conv(features=self.net_width*2, kernel_size=(3, 3))(x)\n    x = nn.relu(x)\n    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n    x = x.reshape((x.shape[0], -1))  # flatten\n    x = nn.Dense(features=self.net_width*4)(x)\n    x = nn.relu(x)\n    x = nn.Dense(features=self.num_classes)(x)\n    return x\n\n\ndef main(argv):\n  # manual flags parsing to avoid conflicts between absl.app.run and sphinx-gallery\n  flags.FLAGS(argv)\n  FLAGS = flags.FLAGS\n\n  # Hide any GPUs from TensorFlow. Otherwise TF might reserve memory and make\n  # it unavailable to JAX.\n  tf.config.experimental.set_visible_devices([], 'GPU')\n\n  train_ds, ds_info = load_dataset(FLAGS.dataset, FLAGS.batch_size)\n\n  # Initialize parameters.\n  input_shape = (1,) + ds_info.features[\"image\"].shape\n  rng = jax.random.PRNGKey(0)\n  num_classes = ds_info.features[\"label\"].num_classes\n  net = CNN(num_classes, FLAGS.net_width)\n\n\n  logistic_loss = jax.vmap(loss.multiclass_logistic_loss)\n  def loss_fun(params, data):\n    \"\"\"Compute the loss of the network.\"\"\"\n    inputs, labels = data\n    x = inputs.astype(jnp.float32) / 255.\n    logits = net.apply({\"params\": params}, x)\n    loss_value = jnp.mean(logistic_loss(labels, logits))\n    return loss_value\n\n\n  def run_all(opt, name):\n    \"\"\"Train the NN on dataset with ``opt``.\"\"\"\n    # Use same starting hyper-parameters for fair evaluation.\n    params = net.init(rng, jnp.zeros(input_shape))[\"params\"]\n\n    # Use same batch order for fair evaluation\n    ds_iterator = iter(train_ds)\n    state = opt.init_state(params, data=next(ds_iterator))\n\n    value_and_grad_loss = jax.value_and_grad(loss_fun)\n    errors, steps, losses = [], [], []\n\n    start = timer()\n    for iter_num, batch in zip(range(opt.maxiter+1), ds_iterator):\n      loss, _ = value_and_grad_loss(params, data=batch)\n      params, state = opt.update(params, state, batch)\n      if \"Armijo\" in name or \"Polyak\" in name:\n        steps.append(state.stepsize)\n      errors.append(state.error)\n      losses.append(loss)\n\n      if iter_num % 20 == 0:\n        if \"Armijo\" in name or \"Polyak\" in name:\n          print(f\"[{name}] ({iter_num:4d}), error={errors[-1]:.3f}, loss={losses[-1]:.3f}, stepsize={steps[-1]:.6f}\")\n        else:\n          print(f\"[{name}] ({iter_num:4d}), error={errors[-1]:.3f}, loss={losses[-1]:.3f}\")\n    duration = timer() - start\n    print(f\"{opt.maxiter+1} iterations of {name} took {duration:.2f}s\\n\")\n\n    losses = convolve(losses, onp.ones(5) / 5, mode='valid', method='direct')  # smooth the curve\n    return errors[:len(losses)], steps[:len(losses)], losses\n\n  polyak = PolyakSGD(fun=loss_fun, max_stepsize=FLAGS.max_stepsize, maxiter=FLAGS.maxiter)\n  armijo = ArmijoSGD(fun=loss_fun, aggressiveness=FLAGS.aggressiveness, reset_option='goldstein',\n                     max_stepsize=FLAGS.max_stepsize, maxls=20, maxiter=FLAGS.maxiter)\n  rmsprop = OptaxSolver(fun=loss_fun, maxiter=FLAGS.maxiter,\n                        opt=optax.rmsprop(learning_rate=FLAGS.rmsprop_stepsize))\n  gd = OptaxSolver(fun=loss_fun, maxiter=FLAGS.maxiter,\n                   opt=optax.sgd(learning_rate=FLAGS.gd_stepsize, nesterov=False))\n\n  errors_data, losses_data, stepsize_data = [], [], []\n  for opt, name in zip([polyak, armijo, rmsprop, gd], [\"Polyak SGD\", \"Armijo SGD\", \"RMSProp\", \"SGD\"]):\n    errors, steps, losses = run_all(opt, name)\n    errors_data.append(errors)\n    losses_data.append(losses)\n    stepsize_data.append(steps)\n\n  fig = plt.figure(figsize=(18,12))\n  fig.suptitle(f'Classification on {FLAGS.dataset}')\n  spec = fig.add_gridspec(ncols=2, nrows=2)\n\n  spec_slices = [spec[0,:], spec[1:,:]]\n  datas = stepsize_data, losses_data\n  names = ['Stepsize', 'Training loss']\n  for spec_slice, single_data, name in zip(spec_slices, datas, names):\n    polyak_data, armijo_data, rmsprop_data, gd_data = single_data\n    ax = fig.add_subplot(spec_slice)\n    ax.set_xlabel(\"Iter num\")\n    ax.set_ylabel(f\"{name} (logscale)\")\n    iter_nums = jnp.arange(1, len(polyak_data)+1)\n    ax.plot(iter_nums, polyak_data, ':', c='red', linewidth=1., label=f'Polyak max_stepsize={FLAGS.max_stepsize}')\n    ax.plot(iter_nums, armijo_data, '--', c='green', linewidth=1., label=f'Armijo aggressiveness={FLAGS.aggressiveness}')\n    if name != 'Stepsize':\n      ax.plot(iter_nums, gd_data, '-', c='blue', linewidth=1., label=f'SGD stepsize={FLAGS.gd_stepsize:.3f}')\n      ax.plot(iter_nums, rmsprop_data, '.-', c='orange', linewidth=1., label=f'RMSProp stepsize={FLAGS.rmsprop_stepsize:.3f}')\n    ax.set_yscale('log')\n\n  leg = ax.legend(loc='upper right', framealpha=1.)\n  spec.tight_layout(fig)\n  plt.show()\n\n\nif __name__ == \"__main__\":\n  main(sys.argv)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}