{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Anderson acceleration of gradient descent.\n\nFor a strictly convex function f, $\\nabla f(x)=0$ implies that $x$\nis the global optimum $f$.\n\nConsequently the fixed point of $T(x)=x-\\eta\\nabla f(x)$ is the optimum of\n$f$.\n\nNote that repeated application of the operator $T$ coincides exactlty with\ngradient descent with constant step size $\\eta$.\n\nHence, as any other fixed point iteration, gradient descent can benefit from\nAnderson acceleration. Here, we choose $f$ as the objective function\nof ridge regression on some dummy dataset.  Anderson acceleration reaches the\noptimal parameters within few iterations, whereas gradient descent is slower.\n\nHere `m` denotes the history size, and `K` the frequency of Anderson updates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import jax\nimport jax.numpy as jnp\n\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n\nfrom jaxopt import AndersonAcceleration\nfrom jaxopt import FixedPointIteration\n\nfrom jaxopt import objective\nfrom jaxopt.tree_util import tree_scalar_mul, tree_sub\n\njax.config.update(\"jax_platform_name\", \"cpu\")\n\n\n# retrieve intermediate iterates.\ndef run_all(solver, w_init, *args, **kwargs):\n  state = solver.init_state(w_init, *args, **kwargs)\n  sol = w_init\n  sols, errors = [], []\n\n  for _ in range(solver.maxiter):\n    sol, state = solver.update(sol, state, *args, **kwargs)\n    sols.append(sol)\n    errors.append(state.error)\n\n  return jnp.stack(sols, axis=0), errors\n\n\n# dummy dataset\nX, y = datasets.make_regression(n_samples=100, n_features=10, random_state=0)\nridge_regression_grad = jax.grad(objective.ridge_regression)\n\n# gradient step: x - grad_x f(x) with f the cost of learning task\n# the fixed point of this mapping verifies grad_x f(x) = 0\n# i.e the fixed point is an optimum\ndef T(params, eta, l2reg, data):\n  g = ridge_regression_grad(params, l2reg, data)\n  step = tree_scalar_mul(eta, g)\n  return tree_sub(params, step)\n\nw_init = jnp.zeros(X.shape[1])  # null vector\neta = 1e-1  # small step size\nl2reg = 0.  # no regularization\ntol = 1e-5\nmaxiter = 80\naa = AndersonAcceleration(T, history_size=5, mixing_frequency=1, maxiter=maxiter, ridge=5e-5, tol=tol)\naam = AndersonAcceleration(T, history_size=5, mixing_frequency=5, maxiter=maxiter, ridge=5e-5, tol=tol)\nfpi = FixedPointIteration(T, maxiter=maxiter, tol=tol)\n\naa_sols, aa_errors = run_all(aa, w_init, eta, l2reg, (X, y))\naam_sols, aam_errors = run_all(aam, w_init, eta, l2reg, (X, y))\nfp_sols, fp_errors = run_all(fpi, w_init, eta, l2reg, (X, y))\n\nsol = aa_sols[-1]\nprint(f'Error={aa_errors[-1]:.6f} at parameters {sol}')\nprint(f'At this point the gradient {ridge_regression_grad(sol, l2reg, (X,y))} is close to zero vector so we found the minimum.')\n\nfig = plt.figure(figsize=(10, 12))\nfig.suptitle('Trajectory in parameter space')\nspec = fig.add_gridspec(ncols=2, nrows=3, hspace=0.3)\n\n# Plot trajectory in parameter space (8 dimensions)\nfor i in range(4):\n  ax = fig.add_subplot(spec[i//2, i%2])\n  ax.plot(fp_sols[:,i], fp_sols[:,2*i+1], '-', linewidth=4., label=\"Gradient Descent\")\n  ax.plot(aa_sols[:,i], aa_sols[:,2*i+1], 'v', markersize=12, label=\"Anderson Accelerated GD (m=5, K=1)\")\n  ax.plot(aam_sols[:,i], aam_sols[:,2*i+1], '*', markersize=8, label=\"Anderson Accelerated GD (m=5, K=5)\")\n  ax.set_xlabel(f'$x_{{{2*i+1}}}$')\n  ax.set_ylabel(f'$x_{{{2*i+2}}}$')\n  if i == 0:\n    ax.legend(loc='upper left', bbox_to_anchor=(0.75, 1.38),\n              ncol=1, fancybox=True, shadow=True)\n  ax.axis('equal')\n\n# Plot error as function of iteration num\nax = fig.add_subplot(spec[2, :])\niters = jnp.arange(len(aa_errors))\nax.plot(iters, fp_errors, linewidth=4., label='Gradient Descent Error')\nax.plot(iters, aa_errors, linewidth=4., label='Anderson Accelerated GD Error (m=5, K=1)')\nax.plot(iters, aam_errors, linewidth=4., label='Anderson Accelerated GD Error (m=5, K=5)')\nax.set_xlabel('Iteration num')\nax.set_ylabel('Error')\nax.set_yscale('log')\nax.legend()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}