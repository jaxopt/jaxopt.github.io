{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Robust training.\n\nThe following code trains a convolutional neural network (CNN) to be robust\nwith respect to the projected gradient descent (PGD) method.\n\nThe Projected Gradient Descent Method (PGD) is a simple yet effective method to\ngenerate adversarial images. At each iteration, it adds a small perturbation\nin the direction of the sign of the gradient with respect to the input followed\nby a projection onto the infinity ball. The gradient sign ensures this\nperturbation locally maximizes the objective, while the projection ensures this\nperturbation stays on the boundary of the infinity ball.\n\n## References\n  Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. \"Explaining\n  and harnessing adversarial examples.\" https://arxiv.org/abs/1412.6572\n\n  Madry, Aleksander, et al. \"Towards deep learning models resistant to\n  adversarial attacks.\" arXiv preprint arXiv:1706.06083 (2017).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import datetime\n\nfrom absl import app\nfrom absl import flags\nfrom flax import linen as nn\nimport jax\nfrom jax import numpy as jnp\nfrom jaxopt import loss\nfrom jaxopt import OptaxSolver\nfrom jaxopt import tree_util\nfrom matplotlib import pyplot as plt\nimport optax\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\n\ndataset_names = [\n    \"mnist\", \"kmnist\", \"emnist\", \"fashion_mnist\", \"cifar10\", \"cifar100\"\n]\n\nflags.DEFINE_float(\"l2reg\", 1e-4, \"L2 regularization.\")\nflags.DEFINE_float(\"learning_rate\", 0.001, \"Learning rate.\")\nflags.DEFINE_float(\n    \"epsilon\", 0.01,\n    \"Adversarial perturbations will be constrained to lie within the L-infinity ball of radius epsilon.\"\n)\nflags.DEFINE_enum(\"dataset\", \"mnist\", dataset_names, \"Dataset to train on.\")\nflags.DEFINE_integer(\"epochs\", 2, \"Number of passes over the dataset.\")\nflags.DEFINE_integer(\"train_batch_size\", 256, \"Batch size at train time.\")\nflags.DEFINE_integer(\"test_batch_size\", 256, \"Batch size at test time.\")\n\nFLAGS = flags.FLAGS\n\n\ndef load_dataset(split, *, is_training, batch_size):\n  \"\"\"Load dataset using tensorflow_datasets.\"\"\"\n  version = 3\n  ds, ds_info = tfds.load(\n      f\"{FLAGS.dataset}:{version}.*.*\",\n      as_supervised=True,  # remove useless keys\n      split=split,\n      with_info=True)\n  ds = ds.cache().repeat()\n  if is_training:\n    ds = ds.shuffle(10 * batch_size, seed=0)\n  ds = ds.batch(batch_size)\n  return iter(tfds.as_numpy(ds)), ds_info\n\n\nclass CNN(nn.Module):\n  \"\"\"A simple CNN model.\"\"\"\n  num_classes: int\n\n  @nn.compact\n  def __call__(self, x):\n    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n    x = nn.relu(x)\n    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n    x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n    x = nn.relu(x)\n    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n    x = x.reshape((x.shape[0], -1))  # flatten\n    x = nn.Dense(features=256)(x)\n    x = nn.relu(x)\n    x = nn.Dense(features=self.num_classes)(x)\n    return x\n\n\ndef main(argv):\n  del argv\n\n  # Hide any GPUs from TensorFlow. Otherwise TF might reserve memory and make\n  # it unavailable to JAX.\n  tf.config.experimental.set_visible_devices([], \"GPU\")\n  train_ds, ds_info = load_dataset(\"train\", is_training=True,\n                                   batch_size=FLAGS.train_batch_size)\n  test_ds, _ = load_dataset(\"test\", is_training=False,\n                            batch_size=FLAGS.test_batch_size)\n  input_shape = (1,) + ds_info.features[\"image\"].shape\n  num_classes = ds_info.features[\"label\"].num_classes\n  iter_per_epoch_train = ds_info.splits['train'].num_examples // FLAGS.train_batch_size\n  iter_per_epoch_test = ds_info.splits['test'].num_examples // FLAGS.test_batch_size\n\n\n  net = CNN(num_classes)\n\n  @jax.jit\n  def accuracy(params, data):\n    inputs, labels = data\n    x = inputs.astype(jnp.float32) / 255.\n    logits = net.apply({\"params\": params}, x)\n    return jnp.mean(jnp.argmax(logits, axis=-1) == labels)\n\n  logistic_loss = jax.vmap(loss.multiclass_logistic_loss)\n\n  @jax.jit\n  def loss_fun(params, l2reg, data):\n    \"\"\"Compute the loss of the network.\"\"\"\n    inputs, labels = data\n    x = inputs.astype(jnp.float32) / 255.\n    logits = net.apply({\"params\": params}, x)\n    sqnorm = tree_util.tree_l2_norm(params, squared=True)\n    loss_value = jnp.mean(logistic_loss(labels, logits))\n    return loss_value + 0.5 * l2reg * sqnorm\n\n  @jax.jit\n  def pgd_attack(image, label, params, epsilon=0.1, maxiter=10):\n    \"\"\"PGD attack on the L-infinity ball with radius epsilon.\n\n    Args:\n      image: array-like, input data for the CNN\n      label: integer, class label corresponding to image\n      params: tree, parameters of the model to attack\n      epsilon: float, radius of the L-infinity ball.\n      maxiter: int, number of iterations of this algorithm.\n\n    Returns:\n      perturbed_image: Adversarial image on the boundary of the L-infinity ball\n        of radius epsilon and centered at image.\n\n    Notes:\n      PGD attack is described in (Madry et al. 2017),\n      https://arxiv.org/pdf/1706.06083.pdf\n    \"\"\"\n    image_perturbation = jnp.zeros_like(image)\n    def adversarial_loss(perturbation):\n      return loss_fun(params, 0, (image + perturbation, label))\n\n    grad_adversarial = jax.grad(adversarial_loss)\n    for _ in range(maxiter):\n      # compute gradient of the loss wrt to the image\n      sign_grad = jnp.sign(grad_adversarial(image_perturbation))\n\n      # heuristic step-size 2 eps / maxiter\n      image_perturbation += (2 * epsilon / maxiter) * sign_grad\n      # projection step onto the L-infinity ball centered at image\n      image_perturbation = jnp.clip(image_perturbation, - epsilon, epsilon)\n\n    # clip the image to ensure pixels are between 0 and 1\n    return jnp.clip(image + image_perturbation, 0, 1)\n\n  # Initialize solver and parameters.\n  solver = OptaxSolver(\n      opt=optax.adam(FLAGS.learning_rate),\n      fun=loss_fun,\n      maxiter=FLAGS.epochs * iter_per_epoch_train)\n  key = jax.random.PRNGKey(0)\n  params = net.init(key, jnp.zeros(input_shape))[\"params\"]\n\n  state = solver.init_state(params)\n  start = datetime.datetime.now().replace(microsecond=0)\n  jitted_update = jax.jit(solver.update)\n\n  accuracy_train = []\n  accuracy_test = []\n  adversarial_accuracy_train = []\n  adversarial_accuracy_test = []\n  for it in range(solver.maxiter):\n    # training loop\n    images, labels = next(train_ds)\n    # convert images to float as attack requires to take gradients wrt to them\n    images = images.astype(jnp.float32) / 255\n\n    adversarial_images_train = pgd_attack(\n        images, labels, params, epsilon=FLAGS.epsilon)\n    # train on adversarial images\n    params, state = jitted_update(\n        params=params,\n        state=state,\n        l2reg=FLAGS.l2reg,\n        data=(adversarial_images_train, labels))\n\n    # Once per epoch evaluate the model on the train and test sets.\n    if state.iter_num % iter_per_epoch_train == iter_per_epoch_train - 1:\n\n      # compute train set accuracy, both on clean and adversarial images\n      adversarial_accuracy_train_sample = 0.\n      accuracy_train_sample = 0.\n      for _ in range(iter_per_epoch_train):\n        images, labels = next(train_ds)\n        images = images.astype(jnp.float32) / 255\n        accuracy_train_sample += jnp.mean(accuracy(params, (images, labels))) / iter_per_epoch_train\n        adversarial_images_train = pgd_attack(\n          images, labels, params, epsilon=FLAGS.epsilon)\n        adversarial_accuracy_train_sample += jnp.mean(accuracy(params, (adversarial_images_train, labels))) / iter_per_epoch_train\n      accuracy_train.append(accuracy_train_sample)\n      adversarial_accuracy_train.append(adversarial_accuracy_train_sample)\n\n      # compute train set accuracy, both on clean and adversarial images\n      adversarial_accuracy_test_sample = 0.\n      accuracy_test_sample = 0.\n      for _ in range(iter_per_epoch_test):\n        images, labels = next(test_ds)\n        images = images.astype(jnp.float32) / 255\n        accuracy_test_sample += jnp.mean(accuracy(params, (images, labels))) / iter_per_epoch_test\n        adversarial_images_test = pgd_attack(\n          images, labels, params, epsilon=FLAGS.epsilon)\n        adversarial_accuracy_test_sample += jnp.mean(accuracy(params, (adversarial_images_test, labels))) / iter_per_epoch_test\n      accuracy_test.append(accuracy_test_sample)\n      adversarial_accuracy_test.append(adversarial_accuracy_test_sample)\n\n\n      time_elapsed = (datetime.datetime.now().replace(microsecond=0) - start)\n      print(f\"Epoch {it // iter_per_epoch_train} out of {FLAGS.epochs}\")\n      print(f\"Accuracy on train set: {accuracy_train[-1]:.3f}\")\n      print(f\"Accuracy on test set: {accuracy_test[-1]:.3f}\")\n      print(\n          f\"Adversarial accuracy on train set: {adversarial_accuracy_train[-1]:.3f}\"\n      )\n      print(\n          f\"Adversarial accuracy on test set: {adversarial_accuracy_test[-1]:.3f}\"\n      )\n      print(f\"Time elapsed: {time_elapsed}\")\n      print()\n\n  plt.title(f\"Adversarial training on {FLAGS.dataset}\")\n  plt.plot(accuracy_train, lw=3, label=\"clean accuracy on train set.\" , marker='<')\n  plt.plot(accuracy_test, lw=3, label=\"clean accuracy on test set.\", marker='d')\n  plt.plot(\n      adversarial_accuracy_train,\n      lw=3,\n      label=\"adversarial accuracy on train set.\", marker='^')\n  plt.plot(\n      adversarial_accuracy_test,\n      lw=3,\n      label=\"adversarial accuracy on test set.\", marker='>')\n  plt.grid()\n  plt.legend(frameon=False, ncol=2)\n  plt.show()\n\n\nif __name__ == \"__main__\":\n  app.run(main)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}