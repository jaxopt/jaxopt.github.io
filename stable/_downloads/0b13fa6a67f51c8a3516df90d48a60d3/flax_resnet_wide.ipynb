{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Resnet example with Flax and JAXopt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from absl import app\nfrom absl import flags\nfrom datetime import datetime\n\nfrom functools import partial\nfrom typing import Any, Callable, Sequence, Tuple\n\nfrom flax import linen as nn\n\nimport jax\nimport jax.numpy as jnp\n\nfrom jaxopt import loss\nfrom jaxopt import OptaxSolver\nfrom jaxopt import tree_util\n\nimport optax\n\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\n\n\nimport ml_collections\n\n\ndataset_names = [\n    \"mnist\", \"kmnist\", \"emnist\", \"fashion_mnist\", \"cifar10\", \"cifar100\"\n]\n\n\nflags.DEFINE_float(\"l2reg\", 1e-4, \"L2 regularization.\")\nflags.DEFINE_float(\"learning_rate\", 0.2, \"Learning rate.\")\nflags.DEFINE_integer(\"epochs\", 10, \"Number of passes over the dataset.\")\nflags.DEFINE_float(\"momentum\", 0.9, \"Momentum strength.\")\nflags.DEFINE_enum(\"dataset\", \"mnist\", dataset_names, \"Dataset to train on.\")\nflags.DEFINE_enum(\"model\", \"resnet18\", [\"resnet1\", \"resnet18\", \"resnet34\"],\n                  \"Model architecture.\")\nflags.DEFINE_integer(\"train_batch_size\", 256, \"Batch size at train time.\")\nflags.DEFINE_integer(\"test_batch_size\", 1024, \"Batch size at test time.\")\nFLAGS = flags.FLAGS\n\n\ndef load_dataset(split, *, is_training, batch_size):\n  version = 3\n  ds, ds_info = tfds.load(\n      f\"{FLAGS.dataset}:{version}.*.*\",\n      as_supervised=True,  # remove useless keys\n      split=split,\n      with_info=True)\n  ds = ds.cache().repeat()\n  if is_training:\n    ds = ds.shuffle(10 * batch_size, seed=0)\n  ds = ds.batch(batch_size)\n  return iter(tfds.as_numpy(ds)), ds_info\n\n\ndef main(argv):\n  del argv\n\n  # Hide any GPUs from TensorFlow. Otherwise TF might reserve memory and make\n  # it unavailable to JAX.\n  tf.config.experimental.set_visible_devices([], 'GPU')\n\n  train_ds, ds_info = load_dataset(\"train\", is_training=True,\n                                   batch_size=FLAGS.train_batch_size)\n  test_ds, _ = load_dataset(\"test\", is_training=False,\n                            batch_size=FLAGS.test_batch_size)\n  input_shape = (1,) + ds_info.features[\"image\"].shape\n  num_classes = ds_info.features[\"label\"].num_classes\n  iter_per_epoch = ds_info.splits['train'].num_examples // FLAGS.train_batch_size\n  iter_per_epoch_test = ds_info.splits['test'].num_examples // FLAGS.test_batch_size\n\n  # Set up model.\n  if FLAGS.model == \"resnet1\":\n    net = ResNet1(num_classes=num_classes)\n  elif FLAGS.model == \"resnet18\":\n    net = ResNet18(num_classes=num_classes)\n  elif FLAGS.model == \"resnet34\":\n    net = ResNet34(num_classes=num_classes)\n  else:\n    raise ValueError(\"Unknown model.\")\n\n  def predict(params, inputs, aux, train=False):\n    x = inputs.astype(jnp.float32) / 255.\n    all_params = {\"params\": params, \"batch_stats\": aux}\n    if train:\n      # Returns logits and net_state (which contains the key \"batch_stats\").\n      return net.apply(all_params, x, train=True, mutable=[\"batch_stats\"])\n    else:\n      # Returns logits only.\n      return net.apply(all_params, x, train=False)\n\n  logistic_loss = jax.vmap(loss.multiclass_logistic_loss)\n\n  def loss_from_logits(params, l2reg, logits, labels):\n    mean_loss = jnp.mean(logistic_loss(labels, logits))\n    sqnorm = tree_util.tree_l2_norm(params, squared=True)\n    return mean_loss + 0.5 * l2reg * sqnorm\n\n  def accuracy_and_loss(params, l2reg, data, aux):\n    inputs, labels = data\n    logits = predict(params, inputs, aux)\n    accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == labels)\n    loss = loss_from_logits(params, l2reg, logits, labels)\n    return accuracy, loss\n\n  def loss_fun(params, l2reg, data, aux):\n    inputs, labels = data\n    logits, net_state = predict(params, inputs, aux, train=True)\n    loss = loss_from_logits(params, l2reg, logits, labels)\n    # batch_stats will be stored in state.aux\n    return loss, net_state[\"batch_stats\"]\n\n  # Initialize solver.\n  config = get_config()\n  base_learning_rate = config.learning_rate * FLAGS.train_batch_size / 256.\n  learning_rate_fn = create_learning_rate_fn(config=config,\n                                             base_learning_rate=base_learning_rate,\n                                             steps_per_epoch=iter_per_epoch)\n\n  #opt = optax.sgd(learning_rate=FLAGS.learning_rate,\n  opt = optax.sgd(learning_rate=learning_rate_fn,\n                  momentum=FLAGS.momentum,\n                  nesterov=True)\n\n  # We need has_aux=True because loss_fun returns batch_stats.\n  solver = OptaxSolver(opt=opt, fun=loss_fun, maxiter=FLAGS.epochs * iter_per_epoch, has_aux=True)\n\n  # Initialize parameters.\n  rng = jax.random.PRNGKey(0)\n  init_vars = net.init(rng, jnp.zeros(input_shape), train=True)\n  params = init_vars[\"params\"]\n  batch_stats = init_vars[\"batch_stats\"]\n  start = datetime.now().replace(microsecond=0)\n\n  # Run training loop.\n  state = solver.init_state(params)\n  jitted_update = jax.jit(solver.update)\n\n  for _ in range(solver.maxiter):\n    train_minibatch = next(train_ds)\n\n    if state.iter_num % iter_per_epoch == iter_per_epoch - 1:\n      # Once per epoch evaluate the model on the train and test sets.\n      train_acc, train_loss = accuracy_and_loss(params, FLAGS.l2reg, train_minibatch, batch_stats)\n      test_acc, test_loss = 0., 0.\n      # make a pass over test set to compute test accuracy\n      for _ in range(iter_per_epoch_test):\n          tmp = accuracy_and_loss(params, FLAGS.l2reg, next(test_ds), batch_stats)\n          test_acc += tmp[0] / iter_per_epoch_test\n          test_loss += tmp[1] / iter_per_epoch_test\n\n      train_acc = jax.device_get(train_acc)\n      train_loss = jax.device_get(train_loss)\n      test_acc = jax.device_get(test_acc)\n      test_loss = jax.device_get(test_loss)\n      # time elapsed without microseconds\n      time_elapsed = (datetime.now().replace(microsecond=0) - start)\n\n      print(f\"[Epoch {state.iter_num // (iter_per_epoch+1)}/{FLAGS.epochs}] \"\n            f\"Train acc: {train_acc:.3f}, train loss: {train_loss:.3f}. \"\n            f\"Test acc: {test_acc:.3f}, test loss: {test_loss:.3f}. \"\n            f\"Time elapsed: {time_elapsed}\")\n\n    params, state = jitted_update(params=params,\n                                  state=state,\n                                  l2reg=FLAGS.l2reg,\n                                  data=train_minibatch,\n                                  aux=batch_stats)\n    batch_stats = state.aux\n\n\nif __name__ == \"__main__\":\n  app.run(main)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}