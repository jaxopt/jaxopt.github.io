{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Image classification example with Haiku and JAXopt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import functools\n\nfrom absl import app\nfrom absl import flags\n\nimport haiku as hk\n\nimport jax\nimport jax.numpy as jnp\n\nfrom jaxopt import loss\nfrom jaxopt import ArmijoSGD\nfrom jaxopt import OptaxSolver\nfrom jaxopt import PolyakSGD\nfrom jaxopt import tree_util\n\nimport optax\n\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\n\n\ndataset_names = [\n    \"mnist\", \"kmnist\", \"emnist\", \"fashion_mnist\", \"cifar10\", \"cifar100\"\n]\n\n\nflags.DEFINE_float(\"l2reg\", 1e-4, \"L2 regularization.\")\nflags.DEFINE_float(\"learning_rate\", 0.001, \"Learning rate (used in adam).\")\nflags.DEFINE_bool(\"manual_loop\", False, \"Whether to use a manual training loop.\")\nflags.DEFINE_integer(\"epochs\", 5, \"Number of passes over the dataset.\")\nflags.DEFINE_float(\"max_stepsize\", 0.1, \"Maximum step size (used in polyak-sgd, armijo-sgd).\")\nflags.DEFINE_float(\"aggressiveness\", 0.5, \"Aggressiveness of line search in armijo-sgd.\")\nflags.DEFINE_float(\"momentum\", 0.9, \"Momentum strength (used in adam, polyak-sgd, armijo-sgd).\")\nflags.DEFINE_enum(\"dataset\", \"mnist\", dataset_names, \"Dataset to train on.\")\nflags.DEFINE_enum(\"model\", \"cnn\", [\"cnn\", \"mlp\"], \"Model architecture.\")\nflags.DEFINE_enum(\"solver\", \"adam\", [\"adam\", \"sgd\", \"polyak-sgd\", \"armijo-sgd\"], \"Solver to use.\")\nflags.DEFINE_integer(\"train_batch_size\", 256, \"Batch size at train time.\")\nflags.DEFINE_integer(\"test_batch_size\", 1024, \"Batch size at test time.\")\nFLAGS = flags.FLAGS\n\n\ndef load_dataset(split, *, is_training, batch_size):\n  \"\"\"Loads the dataset as a generator of batches.\"\"\"\n  version = 3\n  ds, ds_info = tfds.load(\n      f\"{FLAGS.dataset}:{version}.*.*\",\n      as_supervised=True,  # remove useless keys\n      split=split,\n      with_info=True)\n  ds = ds.cache().repeat()\n  if is_training:\n    ds = ds.shuffle(10 * batch_size, seed=0)\n  ds = ds.batch(batch_size)\n  return iter(tfds.as_numpy(ds)), ds_info\n\n\ndef net_fun(batch, num_classes):\n  \"\"\"Create model.\"\"\"\n  x = batch[0].astype(jnp.float32) / 255.\n  if FLAGS.model == \"cnn\":\n    model = hk.Sequential([\n        hk.Conv2D(output_channels=32, kernel_shape=(3, 3), padding=\"SAME\"),\n        jax.nn.relu,\n        hk.AvgPool(window_shape=(2, 2), strides=(2, 2), padding=\"SAME\"),\n        hk.Conv2D(output_channels=64, kernel_shape=(3, 3), padding=\"SAME\"),\n        jax.nn.relu,\n        hk.AvgPool(window_shape=(2, 2), strides=(2, 2), padding=\"SAME\"),\n        hk.Flatten(),\n        hk.Linear(256),\n        jax.nn.relu,\n        hk.Linear(num_classes),\n    ])\n  else:\n    model = hk.Sequential([\n        hk.Flatten(),\n        hk.Linear(300),\n        jax.nn.relu,\n        hk.Linear(100),\n        jax.nn.relu,\n        hk.Linear(num_classes),\n    ])\n  y = model(x)\n  return y\n\n\ndef main(argv):\n  del argv\n\n  # Hide any GPUs from TensorFlow. Otherwise TF might reserve memory and make\n  # it unavailable to JAX.\n  tf.config.experimental.set_visible_devices([], 'GPU')\n\n  train_ds, ds_info = load_dataset(\"train\", is_training=True, batch_size=FLAGS.train_batch_size)\n  test_ds, _ = load_dataset(\"test\", is_training=False, batch_size=FLAGS.test_batch_size)\n  num_classes = ds_info.features[\"label\"].num_classes\n  maxiter = FLAGS.epochs * ds_info.splits['train'].num_examples // FLAGS.train_batch_size\n\n  # Initialize parameters.\n  net = functools.partial(net_fun, num_classes=num_classes)\n  net = hk.without_apply_rng(hk.transform(net))\n\n  logistic_loss = jax.vmap(loss.multiclass_logistic_loss)\n\n  def loss_fun(params, l2reg, data):\n    \"\"\"Compute the loss of the network.\"\"\"\n    logits = net.apply(params, data)\n    _, labels = data\n    sqnorm = tree_util.tree_l2_norm(params, squared=True)\n    loss_value = jnp.mean(logistic_loss(labels, logits))\n    return loss_value + 0.5 * l2reg * sqnorm\n\n  @jax.jit\n  def accuracy(params, data):\n    _, labels = data\n    predictions = net.apply(params, data)\n    return jnp.mean(jnp.argmax(predictions, axis=-1) == labels)\n\n  def print_accuracy(params, state, *args, **kwargs):\n    if state.iter_num % 10 == 0:\n      # Periodically evaluate classification accuracy on test set.\n      test_accuracy = accuracy(params, next(test_ds))\n      test_accuracy = jax.device_get(test_accuracy)\n      print(f\"[Step {state.iter_num}] Test accuracy: {test_accuracy:.3f}.\")\n    return params, state\n\n  # Initialize solver.\n\n  if FLAGS.solver == \"adam\":\n    # Equivalent to:\n    # opt = optax.chain(optax.scale_by_adam(b1=0.9, b2=0.999, eps=1e-8),\n    #                   optax.scale(-FLAGS.learning_rate))\n    opt = optax.adam(FLAGS.learning_rate)\n    solver = OptaxSolver(opt=opt, fun=loss_fun, maxiter=maxiter,\n                         pre_update=print_accuracy)\n\n  elif FLAGS.solver == \"sgd\":\n    opt = optax.sgd(FLAGS.learning_rate, FLAGS.momentum)\n    solver = OptaxSolver(opt=opt, fun=loss_fun, maxiter=maxiter,\n                         pre_update=print_accuracy)\n\n\n  elif FLAGS.solver == \"polyak-sgd\":\n    solver = PolyakSGD(fun=loss_fun, maxiter=maxiter,\n                       momentum=FLAGS.momentum,\n                       max_stepsize=FLAGS.max_stepsize,\n                       pre_update=print_accuracy)\n\n\n  elif FLAGS.solver == \"armijo-sgd\":\n    solver = ArmijoSGD(fun=loss_fun, maxiter=maxiter,\n                       aggressiveness=FLAGS.aggressiveness,\n                       momentum=FLAGS.momentum,\n                       max_stepsize=FLAGS.max_stepsize,\n                       pre_update=print_accuracy)\n\n  else:\n    raise ValueError(\"Unknown solver: %s\" % FLAGS.solver)\n\n  params = net.init(jax.random.PRNGKey(42), next(train_ds))\n\n  # Run training loop.\n\n  # In JAXopt, stochastic solvers can be run either using a manual for loop or\n  # using `run_iterator`. We include both here for demonstration purpose.\n  if FLAGS.manual_loop:\n    state = solver.init_state(params)\n\n    for _ in range(maxiter):\n      params, state = solver.update(params=params, state=state,\n                                    l2reg=FLAGS.l2reg,\n                                    data=next(train_ds))\n\n  else:\n    params, state = solver.run_iterator(\n        init_params=params, iterator=train_ds, l2reg=FLAGS.l2reg)\n\n  print_accuracy(params=params, state=state)\n\n\nif __name__ == \"__main__\":\n  app.run(main)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}