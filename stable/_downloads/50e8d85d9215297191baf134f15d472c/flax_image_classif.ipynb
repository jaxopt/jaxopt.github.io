{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Image classification example with Flax and JAXopt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from absl import app\nfrom absl import flags\n\nfrom flax import linen as nn\n\nimport jax\nimport jax.numpy as jnp\n\nfrom jaxopt import loss\nfrom jaxopt import ArmijoSGD\nfrom jaxopt import OptaxSolver\nfrom jaxopt import PolyakSGD\nfrom jaxopt import tree_util\n\nimport optax\n\nimport tensorflow_datasets as tfds\n\n\ndataset_names = [\n    \"mnist\", \"kmnist\", \"emnist\", \"fashion_mnist\", \"cifar10\", \"cifar100\"\n]\n\n\nflags.DEFINE_float(\"l2reg\", 1e-4, \"L2 regularization.\")\nflags.DEFINE_float(\"learning_rate\", 0.001, \"Learning rate (used in adam).\")\nflags.DEFINE_bool(\"manual_loop\", False, \"Whether to use a manual training loop.\")\nflags.DEFINE_integer(\"maxiter\", 100, \"Maximum number of iterations.\")\nflags.DEFINE_float(\"aggressiveness\", 0.5, \"Aggressiveness of line search in armijo-sgd.\")\nflags.DEFINE_float(\"momentum\", 0.9, \"Momentum strength (used in adam, polyak-sgd, armijo-sgd).\")\nflags.DEFINE_float(\"max_stepsize\", 0.1, \"Maximum step size (used in polyak-sgd, armijo-sgd).\")\nflags.DEFINE_enum(\"dataset\", \"mnist\", dataset_names, \"Dataset to train on.\")\nflags.DEFINE_enum(\"model\", \"cnn\", [\"cnn\", \"mlp\"], \"Model architecture.\")\nflags.DEFINE_enum(\"solver\", \"adam\", [\"adam\", \"sgd\", \"polyak-sgd\", \"armijo-sgd\"], \"Solver to use.\")\nFLAGS = flags.FLAGS\n\n\ndef load_dataset(split, *, is_training, batch_size):\n  version = 3\n  ds, ds_info = tfds.load(\n      f\"{FLAGS.dataset}:{version}.*.*\",\n      as_supervised=True,  # remove useless keys\n      split=split,\n      with_info=True)\n  ds = ds.cache().repeat()\n  if is_training:\n    ds = ds.shuffle(10 * batch_size, seed=0)\n  ds = ds.batch(batch_size)\n  return iter(tfds.as_numpy(ds)), ds_info\n\n\nclass CNN(nn.Module):\n  \"\"\"A simple CNN model.\"\"\"\n  num_classes: int\n\n  @nn.compact\n  def __call__(self, x):\n    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n    x = nn.relu(x)\n    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n    x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n    x = nn.relu(x)\n    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n    x = x.reshape((x.shape[0], -1))  # flatten\n    x = nn.Dense(features=256)(x)\n    x = nn.relu(x)\n    x = nn.Dense(features=self.num_classes)(x)\n    return x\n\n\nclass MLP(nn.Module):\n  \"\"\"MLP model.\"\"\"\n  num_classes: int\n\n  @nn.compact\n  def __call__(self, x):\n    x = x.reshape((x.shape[0], -1))  # flatten\n    x = nn.Dense(features=300)(x)\n    x = nn.relu(x)\n    x = nn.Dense(features=100)(x)\n    x = nn.relu(x)\n    x = nn.Dense(features=self.num_classes)(x)\n    return x\n\n\ndef main(argv):\n  del argv\n\n  train_ds, ds_info = load_dataset(\"train\", is_training=True, batch_size=256)\n  test_ds, _ = load_dataset(\"test\", is_training=False, batch_size=1024)\n  input_shape = (1,) + ds_info.features[\"image\"].shape\n  num_classes = ds_info.features[\"label\"].num_classes\n\n  # Initialize parameters.\n  if FLAGS.model == \"cnn\":\n    net = CNN(num_classes)\n  else:\n    net = MLP(num_classes)\n  rng = jax.random.PRNGKey(0)\n  params = net.init(rng, jnp.zeros(input_shape))[\"params\"]\n\n  @jax.jit\n  def accuracy(params, data):\n    inputs, labels = data\n    x = inputs.astype(jnp.float32) / 255.\n    logits = net.apply({\"params\": params}, x)\n    return jnp.mean(jnp.argmax(logits, axis=-1) == labels)\n\n  logistic_loss = jax.vmap(loss.multiclass_logistic_loss)\n\n  def loss_fun(params, l2reg, data):\n    \"\"\"Compute the loss of the network.\"\"\"\n    inputs, labels = data\n    x = inputs.astype(jnp.float32) / 255.\n    logits = net.apply({\"params\": params}, x)\n    sqnorm = tree_util.tree_l2_norm(params, squared=True)\n    loss_value = jnp.mean(logistic_loss(labels, logits))\n    return loss_value + 0.5 * l2reg * sqnorm\n\n  def print_accuracy(params, state, *args, **kwargs):\n    if state.iter_num % 10 == 0:\n      # Periodically evaluate classification accuracy on test set.\n      test_accuracy = accuracy(params, next(test_ds))\n      test_accuracy = jax.device_get(test_accuracy)\n      print(f\"[Step {state.iter_num}] Test accuracy: {test_accuracy:.3f}.\")\n    return params, state\n\n  # Initialize solver.\n  if FLAGS.solver == \"adam\":\n    solver = OptaxSolver(opt=optax.adam(1e-3), fun=loss_fun,\n                         maxiter=FLAGS.maxiter, pre_update=print_accuracy)\n\n  elif FLAGS.solver == \"sgd\":\n    opt = optax.sgd(FLAGS.learning_rate, FLAGS.momentum)\n    solver = OptaxSolver(opt=opt, fun=loss_fun,\n                         maxiter=FLAGS.maxiter, pre_update=print_accuracy)\n\n\n  elif FLAGS.solver == \"polyak-sgd\":\n    solver = PolyakSGD(fun=loss_fun, maxiter=FLAGS.maxiter,\n                       momentum=FLAGS.momentum,\n                       max_stepsize=FLAGS.max_stepsize,\n                       pre_update=print_accuracy)\n\n\n  elif FLAGS.solver == \"armijo-sgd\":\n    solver = ArmijoSGD(fun=loss_fun, maxiter=FLAGS.maxiter,\n                       aggressiveness=FLAGS.aggressiveness,\n                       momentum=FLAGS.momentum,\n                       max_stepsize=FLAGS.max_stepsize,\n                       pre_update=print_accuracy)\n\n  else:\n    raise ValueError(\"Unknown solver: %s\" % FLAGS.solver)\n\n  # Run training loop.\n\n  # In JAXopt, stochastic solvers can be run either using a manual for loop or\n  # using `run_iterator`. We include both here for demonstration purpose.\n  if FLAGS.manual_loop:\n    state = solver.init_state(params)\n\n    for _ in range(FLAGS.maxiter):\n      params, state = solver.update(params=params, state=state,\n                                    l2reg=FLAGS.l2reg,\n                                    data=next(train_ds))\n\n  else:\n    params, state = solver.run_iterator(\n        init_params=params, iterator=train_ds, l2reg=FLAGS.l2reg)\n\n    print_accuracy(params=params, state=state)\n\nif __name__ == \"__main__\":\n  app.run(main)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}