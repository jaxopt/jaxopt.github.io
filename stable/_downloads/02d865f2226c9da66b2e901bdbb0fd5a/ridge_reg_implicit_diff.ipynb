{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Implicit differentiation of ridge regression.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from absl import app\nimport jax\nimport jax.numpy as jnp\nfrom jaxopt import implicit_diff\nfrom jaxopt import linear_solve\nfrom jaxopt import OptaxSolver\nimport optax\nfrom sklearn import datasets\nfrom sklearn import model_selection\nfrom sklearn import preprocessing\n\n\ndef ridge_objective(params, l2reg, data):\n  \"\"\"Ridge objective function.\"\"\"\n  X_tr, y_tr = data\n  residuals = jnp.dot(X_tr, params) - y_tr\n  return 0.5 * jnp.mean(residuals ** 2) + 0.5 * l2reg * jnp.sum(params ** 2)\n\n\n@implicit_diff.custom_root(jax.grad(ridge_objective))\ndef ridge_solver(init_params, l2reg, data):\n  \"\"\"Solve ridge regression by conjugate gradient.\"\"\"\n  X_tr, y_tr = data\n\n  def matvec(u):\n    return jnp.dot(X_tr.T, jnp.dot(X_tr, u))\n\n  return linear_solve.solve_cg(matvec=matvec,\n                               b=jnp.dot(X_tr.T, y_tr),\n                               ridge=len(y_tr) * l2reg,\n                               init=init_params,\n                               maxiter=20)\n\n\n# Perhaps confusingly, theta is a parameter of the outer objective,\n# but l2reg = jnp.exp(theta) is an hyper-parameter of the inner objective.\ndef outer_objective(theta, init_inner, data):\n  \"\"\"Validation loss.\"\"\"\n  X_tr, X_val, y_tr, y_val = data\n  # We use the bijective mapping l2reg = jnp.exp(theta)\n  # both to optimize in log-space and to ensure positivity.\n  l2reg = jnp.exp(theta)\n  w_fit = ridge_solver(init_inner, l2reg, (X_tr, y_tr))\n  y_pred = jnp.dot(X_val, w_fit)\n  loss_value = jnp.mean((y_pred - y_val) ** 2)\n  # We return w_fit as auxiliary data.\n  # Auxiliary data is stored in the optimizer state (see below).\n  return loss_value, w_fit\n\n\ndef main(argv):\n  del argv\n\n  # Prepare data.\n  X, y = datasets.load_boston(return_X_y=True)\n  X = preprocessing.normalize(X)\n  # data = (X_tr, X_val, y_tr, y_val)\n  data = model_selection.train_test_split(X, y, test_size=0.33, random_state=0)\n\n  # Initialize solver.\n  solver = OptaxSolver(opt=optax.adam(1e-2), fun=outer_objective, has_aux=True)\n  theta_init = 1.0\n  theta, state = solver.init(theta_init)\n  init_w = jnp.zeros(X.shape[1])\n\n  # Run outer loop.\n  for _ in range(50):\n    theta, state = solver.update(params=theta, state=state, init_inner=init_w,\n                                 data=data)\n    # The auxiliary data returned by the outer loss is stored in the state.\n    init_w = state.aux\n    print(f\"[Step {state.iter_num}] Validation loss: {state.value:.3f}.\")\n\nif __name__ == \"__main__\":\n  app.run(main)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}