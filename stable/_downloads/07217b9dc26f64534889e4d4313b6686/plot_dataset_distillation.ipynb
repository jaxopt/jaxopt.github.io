{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Dataset Distillation Example\n\nDataset distillation `[Maclaurin et al. 2015]\n<https://arxiv.org/pdf/1502.03492.pdf>`_, `[Wang et al. 2020]\n<https://arxiv.org/pdf/1811.10959.pdf>`_) aims to learn a small synthetic\ntraining dataset such that a model trained on this learned data set achieves\nsmall loss on the original training set.\n\n## Bi-level formulation\n\nDataset distillation can be written formally as a bi-level problem, where in the\ninner problem we estimate a logistic regression model $x^\\star(\\theta) \\in\n\\mathbb{R}^{p \\times k}$ trained on the distilled images $\\theta \\in\n\\mathbb{R}^{k \\times p}$, while in the outer problem we want to minimize the\nloss achieved by $x^\\star(\\theta)$ over the training set:\n\n\\begin{align}\\underbrace{\\min_{\\theta \\in \\mathbb{R}^{k \\times p}} f(x^\\star(\\theta),\n    X_{\\text{tr}}; y_{\\text{tr}})}_{\\text{outer problem}} ~\\text{ subject to }~\n    x^\\star(\\theta) \\in \\underbrace{\\text{argmin}_{x \\in \\mathbb{R}^{p \\times k}}\n    f(x, \\theta; [k]) + \\varepsilon \\|x\\|^2\\,}_{\\text{inner problem}},\\end{align}\n\nwhere $f(W, X; y) := \\ell(y, XW)$, and $\\ell$ denotes the multiclass\nlogistic regression loss, and $\\varepsilon = 10^{-3}$ is a regularization\nparameter that we found improved convergence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\nfrom matplotlib import pyplot as plt\n\nimport jax\nfrom jax import numpy as jnp\n\nfrom jaxopt import GradientDescent\nfrom jaxopt import objective\n\n# load mnist\nmnist_train, ds_info = tfds.load(name=\"mnist\", split=\"train\", with_info=True)\nimages_train = jnp.array([ex['image'].ravel() for ex in tfds.as_numpy(mnist_train)]) / 255.0\nlabels_train = jnp.array([ex['label'] for ex in tfds.as_numpy(mnist_train)])\n\nmnist_test = tfds.load(name=\"mnist\", split=\"test\")\nimages_test = jnp.array([ex['image'].ravel() for ex in tfds.as_numpy(mnist_test)]) / 255.0\nlabels_test = jnp.array([ex['label'] for ex in tfds.as_numpy(mnist_test)])\n\n\n# we now construct the outer loss and perform gradient descent on the outer loss\n\n# these are the parameters of the logistic regression problem (inner problem)\nparams = jnp.ones((28 * 28, 10))\n\nrng = jax.random.PRNGKey(0)\n\n# distilled images (initialized at random, to be learned). These are the\n# parameters of the outer problem\ndistilled_images = jax.random.normal(rng, (10, 28 * 28)) / (28 * 28)\ndistilled_labels = jnp.arange(10)\n\n# amount of L2 reglarization of the inner problem. This helps both the\n# convergence of the inner problem and the computation of the hypergradient\nl2reg = 1e-1\n\ninner_loss = objective.l2_multiclass_logreg\ngd = GradientDescent(fun=inner_loss, tol=1e-3, maxiter=500)\n\ndef outer_loss(img):\n    # inner_sol is the solution to the inner problem, which computes the\n    # model trained on the 10 images from distilled_images. This makes\n    # the problem bi-level, as the objective depends itself on the solution\n    # of an optimization problem  (inner_sol)\n    inner_sol = gd.run(params, l2reg, (img, distilled_labels)).params\n    return objective.l2_multiclass_logreg(\n        inner_sol, 0, (images_train, labels_train))\n\ngd_outer = GradientDescent(fun=outer_loss, tol=1e-3, maxiter=50)\ndistilled_images, _ = gd_outer.run(distilled_images)\n\nfig, axarr = plt.subplots(1, 10, figsize=(10 * 10, 1 * 10))\nplt.suptitle(\n    \"Distilled images\", fontsize=40)\nfor i in range(10):\n    img_i = distilled_images[i].reshape((28, 28))\n    axarr[i].imshow(\n        img_i / jnp.abs(img_i).max(), cmap=plt.cm.gray_r, vmin=-1, vmax=1)\n    axarr[i].set_xticks(())\n    axarr[i].set_yticks(())\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}