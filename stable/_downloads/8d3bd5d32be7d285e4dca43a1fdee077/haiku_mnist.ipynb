{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# MNIST example with Haiku and JAXopt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from absl import app\nfrom absl import flags\n\nimport haiku as hk\n\nimport jax\nimport jax.numpy as jnp\n\nfrom jaxopt import loss\nfrom jaxopt import OptaxSolver\nfrom jaxopt import PolyakSGD\nfrom jaxopt import tree_util\n\nimport optax\n\nimport tensorflow_datasets as tfds\n\n\nflags.DEFINE_float(\"l2reg\", 1e-4, \"L2 regularization.\")\nflags.DEFINE_float(\"learning_rate\", 0.001, \"Learning rate (used in adam).\")\nflags.DEFINE_bool(\"manual_loop\", False, \"Whether to use a manual training loop.\")\nflags.DEFINE_integer(\"maxiter\", 100, \"Maximum number of iterations.\")\nflags.DEFINE_float(\"max_step_size\", 0.1, \"Maximum step size (used in polyak-sgd).\")\nflags.DEFINE_float(\"momentum\", 0.9, \"Momentum strength (used in adam, polyak-sgd).\")\nflags.DEFINE_enum(\"solver\", \"adam\", [\"adam\", \"sgd\", \"polyak-sgd\"], \"Solver to use.\")\nFLAGS = flags.FLAGS\n\n\ndef load_dataset(split, *, is_training, batch_size):\n  \"\"\"Loads the dataset as a generator of batches.\"\"\"\n  ds = tfds.load(\"mnist:3.*.*\", split=split).cache().repeat()\n  if is_training:\n    ds = ds.shuffle(10 * batch_size, seed=0)\n  ds = ds.batch(batch_size)\n  return iter(tfds.as_numpy(ds))\n\n\ndef net_fun(batch):\n  \"\"\"Standard LeNet-300-100 MLP network.\"\"\"\n  x = batch[\"image\"].astype(jnp.float32) / 255.\n  mlp = hk.Sequential([\n      hk.Flatten(),\n      hk.Linear(300), jax.nn.relu,\n      hk.Linear(100), jax.nn.relu,\n      hk.Linear(10),\n  ])\n  return mlp(x)\n\n\nnet = hk.without_apply_rng(hk.transform(net_fun))\n\n\n@jax.jit\ndef accuracy(params, data):\n  predictions = net.apply(params, data)\n  return jnp.mean(jnp.argmax(predictions, axis=-1) == data[\"label\"])\n\n\nlogistic_loss = jax.vmap(loss.multiclass_logistic_loss)\n\n\ndef loss_fun(params, l2reg, data):\n  \"\"\"Compute the loss of the network.\"\"\"\n  logits = net.apply(params, data)\n  labels = data[\"label\"]\n  sqnorm = tree_util.tree_l2_norm(params, squared=True)\n  loss_value = jnp.mean(logistic_loss(labels, logits))\n  return loss_value + 0.5 * l2reg * sqnorm\n\n\ndef main(argv):\n  del argv\n\n  train_ds = load_dataset(\"train\", is_training=True, batch_size=1000)\n  test_ds = load_dataset(\"test\", is_training=False, batch_size=10000)\n\n  def pre_update(params, state, *args, **kwargs):\n    if state.iter_num % 10 == 0:\n      # Periodically evaluate classification accuracy on test set.\n      test_accuracy = accuracy(params, next(test_ds))\n      test_accuracy = jax.device_get(test_accuracy)\n      print(f\"[Step {state.iter_num}] Test accuracy: {test_accuracy:.3f}.\")\n    return params, state\n\n  # Initialize solver and parameters.\n\n  if FLAGS.solver == \"adam\":\n    # Equilent to:\n    # opt = optax.chain(optax.scale_by_adam(b1=0.9, b2=0.999, eps=1e-8),\n    #                   optax.scale(-FLAGS.learning_rate))\n    opt = optax.adam(FLAGS.learning_rate)\n    solver = OptaxSolver(opt=opt, fun=loss_fun, maxiter=FLAGS.maxiter,\n                         pre_update=pre_update)\n\n  elif FLAGS.solver == \"sgd\":\n    opt = optax.sgd(FLAGS.learning_rate, FLAGS.momentum)\n    solver = OptaxSolver(opt=opt, fun=loss_fun, maxiter=FLAGS.maxiter,\n                         pre_update=pre_update)\n\n\n  elif FLAGS.solver == \"polyak-sgd\":\n    solver = PolyakSGD(fun=loss_fun, maxiter=FLAGS.maxiter,\n                       momentum=FLAGS.momentum,\n                       max_step_size=FLAGS.max_step_size, pre_update=pre_update)\n\n  else:\n    raise ValueError(\"Unknown solver: %s\" % FLAGS.solver)\n\n  init_params = net.init(jax.random.PRNGKey(42), next(train_ds))\n\n  # Run training loop.\n\n  # In JAXopt, stochastic solvers can be run either using a manual for loop or\n  # using `run_iterator`. We include both here for demonstration purpose.\n  if FLAGS.manual_loop:\n    params, state = solver.init(init_params)\n\n    for _ in range(FLAGS.maxiter):\n      params, state = pre_update(params=params, state=state)\n      params, state = solver.update(params=params, state=state,\n                                    l2reg=FLAGS.l2reg,\n                                    data=next(train_ds))\n\n  else:\n    solver.run_iterator(init_params=init_params,\n                        iterator=train_ds,\n                        l2reg=FLAGS.l2reg)\n\n\nif __name__ == \"__main__\":\n  app.run(main)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}