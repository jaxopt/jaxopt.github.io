{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Deep Equilibrium (DEQ) model in Flax with Anderson acceleration.\n\nThis implementation is strongly inspired by the Pytorch code snippets in [3].\n\nA similar model called \"implicit deep learning\" is also proposed in [2].\n\nIn practice BatchNormalization and initialization of weights in convolutions are\nimportant to ensure convergence.\n\n[1] Bai, S., Kolter, J.Z. and Koltun, V., 2019. Deep Equilibrium Models.\nAdvances in Neural Information Processing Systems, 32, pp.690-701.\n\n[2] El Ghaoui, L., Gu, F., Travacca, B., Askari, A. and Tsai, A., 2021.\nImplicit deep learning. SIAM Journal on Mathematics of Data Science, 3(3), pp.930-958.\n\n[3] http://implicit-layers-tutorial.org/deep_equilibrium_models/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from functools import partial\nfrom typing import Any, Mapping, Tuple, Callable\n\nfrom absl import app\nfrom absl import flags\n\nimport flax\nfrom flax import linen as nn\n\nimport jax\nimport jax.numpy as jnp\nfrom jax.tree_util import tree_structure\n\nimport jaxopt\nfrom jaxopt import loss\nfrom jaxopt import OptaxSolver\nfrom jaxopt import FixedPointIteration\nfrom jaxopt import AndersonAcceleration\nfrom jaxopt.linear_solve import solve_gmres, solve_normal_cg\nfrom jaxopt.tree_util import tree_add, tree_sub, tree_l2_norm\n\nimport optax\n\nimport tensorflow_datasets as tfds\nfrom collections import namedtuple\n\n\ndataset_names = [\n    \"mnist\", \"kmnist\", \"emnist\", \"fashion_mnist\", \"cifar10\", \"cifar100\"\n]\n\n# training hyper-parameters\nflags.DEFINE_float(\"l2reg\", 0., \"L2 regularization.\")\nflags.DEFINE_float(\"learning_rate\", 0.001, \"Learning rate.\")\nflags.DEFINE_integer(\"maxiter\", 1000, \"Maximum number of iterations.\")\nflags.DEFINE_enum(\"dataset\", \"cifar10\", dataset_names, \"Dataset to train on.\")\nflags.DEFINE_integer(\"net_width\", 1, \"Multiplicator of neural network width.\")\nflags.DEFINE_integer(\"evaluation_frequency\", 1, \"Number of iterations between two evaluation measures.\")\n\nsolvers = [\"normal_cg\", \"gmres\", \"anderson\"]\nflags.DEFINE_enum(\"backward_solver\", \"normal_cg\", solvers, \"Solver of linear sytem in implicit differentiation.\")\n\n# anderson acceleration parameters\nflags.DEFINE_enum(\"forward_solver\", \"anderson\", [\"anderson\", \"fixed_point\"], 'Whether to use Anderson acceleration.')\nflags.DEFINE_integer(\"forward_maxiter\", 20, \"Number of fixed point iterations.\")\nflags.DEFINE_float(\"forward_tol\", 1e-2, \"Tolerance in fixed point iterations.\")\nflags.DEFINE_integer(\"anderson_history_size\", 5, \"Size of history in Anderson updates.\")\nflags.DEFINE_float(\"anderson_ridge\", 1e-4, \"Ridge regularization in Anderson updates.\")\n\nFLAGS = flags.FLAGS\n\n\ndef load_dataset(split, *, is_training, batch_size):\n  \"\"\"Loads the dataset as a generator of batches.\"\"\"\n  ds, ds_info = tfds.load(f\"{FLAGS.dataset}:3.*.*\", split=split, as_supervised=True, with_info=True)\n  ds = ds.cache().repeat()\n  if is_training:\n    ds = ds.shuffle(10 * batch_size, seed=0)\n  ds = ds.batch(batch_size)\n  return iter(tfds.as_numpy(ds)), ds_info\n\n\nclass ResNetBlock(nn.Module):\n  \"\"\"Fixed Point ResNet block.\"\"\"\n  channels: int\n  channels_bottleneck: int\n  num_groups: int = 8\n  kernel_size: Tuple[int, int] = (3, 3)\n  use_bias: bool = False\n  act: Callable = nn.relu\n\n  @nn.compact\n  def __call__(self, z, x):\n    y = z\n    y = nn.Conv(features=self.channels_bottleneck, kernel_size=self.kernel_size,\n                padding='SAME', use_bias=self.use_bias,\n                kernel_init=jax.nn.initializers.normal(stddev=0.01))(y)\n    y = self.act(y)\n    y = nn.GroupNorm(num_groups=self.num_groups)(y)\n    y = nn.Conv(features=self.channels, kernel_size=self.kernel_size,\n                padding='SAME', use_bias=self.use_bias,\n                kernel_init=jax.nn.initializers.normal(stddev=0.01))(y)\n    y = y + x\n    y = nn.GroupNorm(num_groups=self.num_groups)(y)\n    y = y + z\n    y = self.act(y)\n    y = nn.GroupNorm(num_groups=self.num_groups)(y)\n    return y\n\n\nclass DEQFixedPoint(nn.Module):\n    block: Any  # nn.Module\n    fixed_point_solver: Any  # jaxopt.AndersonAcceleration or jaxopt.FixedPointIteration\n\n    @nn.compact\n    def __call__(self, x):\n        init = lambda rng, x: self.block.init(rng, x[0], x[0])\n        block_params = self.param(\"block_params\", init, x)\n\n        def block_apply(z, x, block_params):\n          return self.block.apply(block_params, z, x)\n\n        solver = self.fixed_point_solver(fixed_point_fun=block_apply)\n        def batch_run(x, block_params):\n          return solver.run(x, x, block_params)[0]\n\n        return jax.vmap(batch_run, in_axes=(0,None), out_axes=0)(x, block_params)\n\n\nclass FullDEQ(nn.Module):\n    num_classes: int\n    channels: int\n    channels_bottleneck: int\n    fixed_point_solver: Callable\n\n    @nn.compact\n    def __call__(self, x, train):\n        x = nn.Conv(features=self.channels, kernel_size=(3,3), use_bias=True, padding='SAME')(x)\n        x = nn.BatchNorm(use_running_average=not train, momentum=0.9, epsilon=1e-5)(x)\n        block = ResNetBlock(self.channels, self.channels_bottleneck)\n        deq_fixed_point = DEQFixedPoint(block, self.fixed_point_solver)\n        x = deq_fixed_point(x)\n        x = nn.BatchNorm(use_running_average=not train, momentum=0.9, epsilon=1e-5)(x)\n        x = nn.avg_pool(x, window_shape=(8,8), padding='SAME')\n        x = x.reshape(x.shape[:-3]+(-1,))  # flatten\n        x = nn.Dense(self.num_classes)(x)\n        return x\n\n# For completeness, we also allow Anderson acceleration for solving\n# the implicit differentiation linear system occurring in the backward pass\ndef solve_linear_system_fixed_point(matvec, v):\n  \"\"\"Solve linear system matvec(u) = v.\n\n  The solution u* of the system is the fixed point of:\n    T(u) = matvec(u) + u - v\n  \"\"\"\n  def fixed_point_fun(u):\n    d_1_T_transpose_u = tree_add(matvec(u), u)\n    return tree_sub(d_1_T_transpose_u, v)\n\n  aa = AndersonAcceleration(fixed_point_fun,\n                            history_size=FLAGS.anderson_history_size, tol=1e-2,\n                            ridge=FLAGS.anderson_ridge, maxiter=20)\n  return aa.run(v)[0]\n\n\ndef main(argv):\n    del argv\n\n    # Solver used for implicit differentiation (backward pass)\n    if FLAGS.backward_solver == \"normal_cg\":\n        implicit_solver = partial(solve_normal_cg, tol=1e-2, maxiter=20)\n    elif FLAGS.backward_solver == \"gmres\":\n        implicit_solver = partial(solve_gmres, tol=1e-2, maxiter=20)\n    elif FLAGS.backward_solver == \"anderson\":\n        implicit_solver = solve_linear_system_fixed_point\n\n    # Solver used for fixed point resolution (forward pass)\n    if FLAGS.forward_solver == \"anderson\":\n        fixed_point_solver = partial(AndersonAcceleration,\n                                     history_size=FLAGS.anderson_history_size, ridge=FLAGS.anderson_ridge,\n                                     maxiter=FLAGS.forward_maxiter, tol=FLAGS.forward_tol,\n                                     implicit_diff=True, implicit_diff_solve=implicit_solver)\n    else:\n        fixed_point_solver = partial(FixedPointIteration,\n                                     maxiter=FLAGS.forward_maxiter, tol=FLAGS.forward_tol,\n                                     implicit_diff=True, implicit_diff_solve=implicit_solver)\n\n    train_ds, ds_info = load_dataset(\"train\", is_training=True, batch_size=256)\n    test_ds, _ = load_dataset(\"test\", is_training=False, batch_size=1024)\n    input_shape = (1,) + ds_info.features[\"image\"].shape\n    num_classes = ds_info.features[\"label\"].num_classes\n\n    net = FullDEQ(num_classes, 3*8*FLAGS.net_width, 4*8*FLAGS.net_width, fixed_point_solver)\n\n    @jax.jit\n    def accuracy(all_vars, data):\n        images, labels = data\n        x = images.astype(jnp.float32) / 255.\n        logits = net.apply(all_vars, x, train=False)\n        return jnp.mean(jnp.argmax(logits, axis=-1) == labels)\n\n\n    logistic_loss = jax.vmap(loss.multiclass_logistic_loss)\n    def loss_fun(params, batch_stats, l2reg, data, train):\n        \"\"\"Compute the loss of the network.\"\"\"\n        images, labels = data\n        x = images.astype(jnp.float32) / 255.\n        if train:\n            all_params = {'params':params, 'batch_stats':batch_stats}\n            logits, batch_stats = net.apply(all_params, x, True, mutable=['batch_stats'])\n        else:\n            logits = net.apply({'params':params, 'batch_stats':batch_stats}, x, False)\n        sqnorm = tree_l2_norm(params, squared=True)\n        loss_value = jnp.mean(logistic_loss(labels, logits))\n        loss = loss_value + 0.5 * l2reg * sqnorm\n        if train:\n            return loss, batch_stats\n        return loss\n\n\n    def print_accuracy(params, state):\n        if state.iter_num % FLAGS.evaluation_frequency == 0:\n            # Periodically evaluate classification accuracy on test set.\n            all_vars = {'params':params, 'batch_stats':state.aux['batch_stats']}\n            test_accuracy = accuracy(all_vars, next(test_ds))\n            test_accuracy = jax.device_get(test_accuracy)\n            print(f\"[Step {state.iter_num}] Test accuracy: {test_accuracy:.3f}.\")\n        return params, state\n\n\n    # Initialize solver and parameters.\n    solver = OptaxSolver(opt=optax.adam(FLAGS.learning_rate), fun=loss_fun,\n                         maxiter=FLAGS.maxiter, pre_update=None,\n                         has_aux=True)\n    rng = jax.random.PRNGKey(0)\n    init_vars = net.init(rng, jnp.ones(input_shape), train=True)\n    params = init_vars['params']\n\n    state = solver.init_state(params)\n    state = state._replace(aux=init_vars)\n\n    @jax.jit\n    def jitted_update(params, state, batch_stats, data):\n        return solver.update(params, state, batch_stats, FLAGS.l2reg, data, True)\n\n    for _ in range(solver.maxiter):\n        batch_stats = state.aux['batch_stats']\n        print_accuracy(params, state)\n        params, state = jitted_update(params, state, batch_stats, next(train_ds))\n\n\nif __name__ == \"__main__\":\n    app.run(main)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}