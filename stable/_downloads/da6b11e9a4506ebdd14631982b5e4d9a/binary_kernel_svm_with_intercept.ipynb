{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Binary kernel SVM with intercept.\n\nThe dual objective of binary kernel SVMs with an intercept contains both\nbox constraints and an equality constraint, making it challenging to solve.\nThe state-of-the-art algorithm to solve this objective is SMO (Sequential\nminimal optimization). We nevertheless demonstrate in this example how to solve\nit by projected gradient descent, by projecting on the constraint set\nusing projection_box_section.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from absl import app\nimport jax.numpy as jnp\nfrom jaxopt import projection\nfrom jaxopt import ProjectedGradient\nimport numpy as onp\nfrom sklearn import datasets\nfrom sklearn import preprocessing\nfrom sklearn import svm\n\n\ndef objective_fun(beta, lam, K, y):\n  \"\"\"Dual objective of binary kernel SVMs with intercept.\"\"\"\n  # The dual objective is:\n  # fun(beta) = 0.5 beta^T K beta - beta^T y\n  # subject to\n  # sum(beta) = 0\n  # 0 <= beta_i <= C if y_i = 1\n  # -C <= beta_i <= 0 if y_i = -1\n  # where C = 1.0 / lam\n  return 0.5 * jnp.dot(beta, jnp.dot(K, beta)) - jnp.dot(beta, y)\n\n\ndef binary_kernel_svm_skl(K, y, lam, tol=1e-5):\n  svc = svm.SVC(kernel=\"precomputed\", C=1.0 / lam).fit(K, y)\n  dual_coef = onp.zeros(K.shape[0])\n  dual_coef[svc.support_] = svc.dual_coef_[0]\n  return dual_coef\n\n\ndef main(argv):\n  del argv\n\n  # Prepare data.\n  X, y = datasets.make_classification(n_samples=20, n_features=5,\n                                      n_informative=3, n_classes=2,\n                                      random_state=0)\n  X = preprocessing.Normalizer().fit_transform(X)\n  y = y * 2 - 1  # Transform labels from {0, 1} to {-1, 1}.\n  lam = 1.0\n  C = 1./ lam\n  K = jnp.dot(X, X.T)  # Use a linear kernel.\n\n  # Define projection operator.\n  w = jnp.ones(X.shape[0])\n\n  def proj(beta, C):\n    box_lower = jnp.where(y == 1, 0, -C)\n    box_upper = jnp.where(y == 1, C, 0)\n    proj_params = (box_lower, box_upper, w, 0.0)\n    return projection.projection_box_section(beta, proj_params)\n\n  # Run solver.\n  beta_init = jnp.ones(X.shape[0])\n  solver = ProjectedGradient(fun=objective_fun, projection=proj,\n                            tol=1e-3, maxiter=500)\n  beta_fit = solver.run(beta_init, hyperparams_proj=C, lam=lam, K=K, y=y).params\n\n  # Compare the obtained dual coefficients with sklearn.\n  beta_fit_skl = binary_kernel_svm_skl(K, y, lam)\n  print(beta_fit)\n  print(beta_fit_skl)\n\nif __name__ == \"__main__\":\n  app.run(main)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}