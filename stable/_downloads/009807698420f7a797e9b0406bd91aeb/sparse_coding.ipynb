{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Implementation of sparse coding using jaxopt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import functools\nfrom typing import Optional\nfrom typing import Type\nfrom typing import Mapping\nfrom typing import Any\nfrom typing import Callable\nfrom typing import Tuple\n\nfrom flax import optim\nimport jax\nimport jax.numpy as jnp\nfrom jaxopt import projection\nfrom jaxopt import prox\nfrom jaxopt import proximal_gradient\n\n\ndef dictionary_loss(\n    codes: jnp.ndarray,\n    params: Tuple[jnp.ndarray, jnp.ndarray],\n    reconstruction_loss_fun: Callable[[jnp.ndarray, jnp.ndarray], jnp.ndarray] = None\n    ):\n  \"\"\"Computes reconstruction loss between data and dict/codes using loss fun.\n\n  Args:\n    codes: a samples x components jnp.ndarray of codes.\n    params: Tuple containing dictionary and data matrix.\n    reconstruction_loss_fun: a callable loss(x, y) -> a real number, where\n      x and y are either entries, slices or the matrices themselves.\n      Set to 1/2 squared L2 norm of difference by default.\n\n  Returns:\n    a float, the reconstruction loss.\n  \"\"\"\n  if reconstruction_loss_fun is None:\n    reconstruction_loss_fun = lambda x, y: 0.5 * jnp.sum((x - y)**2)\n\n  dic, X = params\n  X_pred = codes @ dic\n  return reconstruction_loss_fun(X, X_pred)\n\n\ndef make_task_driven_dictionary_learner(\n    task_loss_fun: Optional[Callable[[Any, Any, Any, Any], float]] = None,\n    reconstruction_loss_fun: Optional[Callable[[jnp.ndarray, jnp.ndarray],\n                                               jnp.ndarray]] = None,\n    optimizer_cls: Optional[Type[optim.Optimizer]] = None,\n    optimizer_kw: Mapping[str, Any] = None,\n    sparse_coding_kw: Mapping[str, Any] = None):\n  \"\"\"Makes a task driven sparse dictionary learning solver.\n\n  Args:\n    task_loss_fun: loss as specified on (codes, dict, task_vars, params) that\n      supplements the usual reconstruction loss formulation. If None, only\n      dictionary learning is carried out, i.e. that term is assumed to be 0.\n    reconstruction_loss_fun: entry (or slice-) wise loss function, set to be\n      the Frobenius norm, || . - . ||^2 by default.\n    optimizer_cls: Optimizer to solve for dictionary and task_vars (if auxiliary\n      task is given). Either None, in which case Jaxopt proximal gradient\n      (with sphere projection on dictionary) is used, or a flax\n      optimizer class specifying projection on the sphere explicitly for dic.\n    optimizer_kw: Arguments to be passed to the optimizer class above, or to\n      jaxopt proximal gradient descent.\n    sparse_coding_kw: Jaxopt arguments to be passed to the proximal descent\n      algorithm computing codes, sparse_coding.\n\n  Returns:\n    Function to learn dictionary from data, number of components and\n      elastic net regularization, using initialization for dictionary,\n      parameters for task and task variables initialization.\n  \"\"\"\n  def learner(X: jnp.ndarray,\n              n_components: int,\n              regularization: float,\n              elastic_penalty: float,\n              dict_init: Optional[jnp.ndarray] = None,\n              task_params: jnp.ndarray = None,\n              task_vars_init: jnp.ndarray = None):\n\n    return _task_sparse_dictionary_learning(X, n_components, regularization,\n                                            elastic_penalty, dict_init,\n                                            task_params, task_vars_init,\n                                            reconstruction_loss_fun,\n                                            task_loss_fun,\n                                            optimizer_cls, optimizer_kw,\n                                            sparse_coding_kw)\n\n  return learner\n\n\ndef _task_sparse_dictionary_learning(\n    X: jnp.ndarray,\n    n_components: int,\n    regularization: float,\n    elastic_penalty: float,\n    dict_init: Optional[jnp.ndarray] = None,\n    task_params: jnp.ndarray = None,\n    task_vars_init: jnp.ndarray = None,\n    reconstruction_loss_fun: Callable[[jnp.ndarray, jnp.ndarray],\n                                      jnp.ndarray] = None,\n    task_loss_fun: Callable[[Any, Any, Any, Any], float] = None,\n    optimizer_cls: Optional[Type[optim.Optimizer]] = None,\n    optimizer_kw: Mapping[str, Any] = None,\n    sparse_coding_kw: Mapping[str, Any] = None):\n  \"\"\"Computes task driven dictionary, w. implicitly defined sparse codes.\n\n  Given a N x d data matrix X, solves a bilevel optimization problem by seeking\n  a dictionary dic of size n_components x d such that, defining implicitly\n  codes = sparse_coding(dic, (X, regularization, elastic_penalty))\n  one has that dic minimizes\n  task_loss(codes, dic, task_var, task_params)\n  if such as task_loss was passed on. If None, then task_loss is replaced by\n  dictionary_loss(codes, (dic, X)).\n\n  Args:\n    X: N x d jnp.ndarray, data matrix with N samples of d features.\n    n_components: int, number of atoms in dictionary.\n    regularization: regularization strength of elastic penalty.\n    elastic_penalty: strength of L2 penalty relative to L1.\n    task_params: auxiliary parameters to define task loss, typically data.\n    dict_init: initialization for dictionary; that returned by SVD by default.\n    reconstruction_loss_fun: loss to be applied to compute reconstruction error.\n    task_loss_fun: task driven loss for codes and dictionary using task_vars and\n      task_params.\n    optimizer_cls: flax optimizer class. If None, falls back on jaxopt projected\n      gradient (with sphere normalization constraints). If not None, instantiate\n      that optimizer.\n    optimizer_kw: parameters passed on to optimizer\n    sparse_coding_kw: parameters passed on to jaxopt prox gradient solver.\n\n  Returns:\n    the n_components x d dictionary solution found by the algorithm, as well as\n    codes.\n  \"\"\"\n\n  if dict_init is None:\n    _, _, dict_init = jax.scipy.linalg.svd(X, False)\n    dict_init = dict_init[:n_components, :]\n\n  has_task = task_loss_fun is not None\n\n  # Loss function, dictionary learning in addition to task driven loss\n  def loss_fun(variables, params):\n    dic, task_vars = variables\n    coding_params, task_params = params\n    codes = sparse_coding(\n        dic,\n        coding_params,\n        reconstruction_loss_fun=reconstruction_loss_fun,\n        sparse_coding_kw=sparse_coding_kw)\n\n    if has_task:  # if there is a task, drop loss, replace it with proper value\n      loss = task_loss_fun(codes, dic, task_vars, task_params)\n    else:\n      loss = dictionary_loss(codes, (dic, X), reconstruction_loss_fun)\n    return loss, codes\n\n  init = (dict_init, task_vars_init)\n\n  optimizer_kw = {} if optimizer_kw is None else optimizer_kw\n\n  proj_sphere = lambda x: jax.vmap(projection.projection_l2_sphere)(x)\n  if optimizer_cls is None:\n    # If no optimizer, use jaxopt projected gradient descent.\n\n    # Define projection-prox, here normalize each dict atom by its norm.\n\n    prox_vars = lambda dic_vars, par, s : (\n        proj_sphere(dic_vars[0]), dic_vars[1])\n\n    solver = proximal_gradient.make_solver_fun(\n        fun=loss_fun, prox=prox_vars, has_aux=True,\n        init=init, **optimizer_kw)\n    dic, task_vars = solver(((X, regularization, elastic_penalty), task_params))\n\n  else:\n    maxiter = optimizer_kw.pop('maxiter', 500)  # Pop'd to set loop size.\n    optimizer = optimizer_cls(**optimizer_kw)\n    optimizer = optimizer.create(init)\n\n    # Use implicit jaxopt gradients to inform optimizer's steps.\n    loss_normalized = lambda dic_vars, params: loss_fun(\n        (proj_sphere(dic_vars[0]), dic_vars[1]), params)\n    grad_fn = jax.value_and_grad(loss_normalized, has_aux=True)\n\n    def train_step(optimizer, params):\n      (loss, codes), grad = grad_fn(optimizer.target, params)\n      new_optimizer = optimizer.apply_gradient(grad)\n      return new_optimizer, loss\n\n    # Training body fun.\n    def body_fun(iteration, in_vars):\n      del iteration\n      optimizer, pars = in_vars\n      optimizer, _ = train_step(optimizer, pars)\n      return (optimizer, pars)\n\n    init_val = (optimizer, ((X, regularization, elastic_penalty), task_params))\n\n    # Run fori_loop, this will be converted to a scan.\n    optimizer, _ = jax.lax.fori_loop(0, maxiter, body_fun, init_val)\n\n    dic, task_vars = optimizer.target\n    # Normalize dictionary before returning it.\n    dic = proj_sphere(dic)\n\n  if has_task:\n    return dic, task_vars\n  return dic\n\n\ndef sparse_coding(dic, params, reconstruction_loss_fun=None,\n                  sparse_coding_kw=None, codes_init=None):\n  \"\"\"Computes optimal codes for data X given a dictionary dic.\"\"\"\n  sparse_coding_kw = {} if sparse_coding_kw is None else sparse_coding_kw\n  loss_fun = functools.partial(dictionary_loss,\n                               reconstruction_loss_fun=reconstruction_loss_fun)\n  X, regularization, elastic_penalty = params\n  n_components, _ = dic.shape\n  N, _ = X.shape\n\n  if codes_init is None:\n    codes_init = jnp.zeros((N, n_components))\n\n  solver = proximal_gradient.make_solver_fun(\n      fun=loss_fun,\n      prox=prox.prox_elastic_net,\n      init=codes_init,\n      **sparse_coding_kw)\n\n  codes = solver(params_fun=(dic, X),\n                 params_prox=[regularization, elastic_penalty])\n  return codes"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}